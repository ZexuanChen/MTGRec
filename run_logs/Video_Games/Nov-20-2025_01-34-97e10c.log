Thu 20 Nov 2025 01:34:49 INFO  Device: cuda
Thu 20 Nov 2025 01:34:49 INFO  Config: {'rand_seed': 2024, 'reproducibility': True, 'num_proc': 1, 'data_dir': 'datasets/Video_Games', 'log_dir': 'run_logs/', 'tensorboard_log_dir': 'tensorboard/', 'ckpt_dir': 'ckpt/Video_Games/Nov-20-2025_01-34-754bad', 'stage': 'pretrain', 'pretrained_model': '', 'epoch_per_stage': [60, 20, 20, 20, 20, 20, 20], 'tau': 3.0, 'load_best_for_next_stage': False, 'train_batch_size': 256, 'eval_batch_size': 256, 'lr': 0.005, 'weight_decay': 0.05, 'warmup_steps': 10000, 'steps': None, 'epochs': 200, 'max_grad_norm': 1.0, 'eval_interval': 1, 'save_interval': 10, 'patience': 50, 'topk': [5, 10], 'metrics': ['ndcg', 'recall'], 'val_metric': 'ndcg@10', 'val_ratio': 1.0, 'val_delay': 199, 'n_codebooks': 3, 'codebook_size': 256, 'expand_final': True, 'token_prefix': 'rqvae/sentence-t5-base_256,256,256,256', 'token_suffix': 'sem_ids', 'n_user_tokens': 1, 'max_item_seq_len': 20, 'num_beams': 20, 'test_num_beams': None, 'num_layers': 7, 'num_decoder_layers': 7, 'd_model': 128, 'd_ff': 512, 'num_heads': 4, 'd_kv': 64, 'dropout_rate': 0.1, 'activation_function': 'relu', 'feed_forward_proj': 'relu', 'results_dir': None, 'sem_id_epochs': [9976, 9977, 9978, 9979, 9980, 9981, 9982, 9983, 9984, 9985, 9986, 9987, 9988, 9989, 9990, 9991, 9992, 9993, 9994, 9995, 9996, 9997, 9998, 9999, 10000], 'run_local_time': 'Nov-20-2025_01-34', 'ckpt_name': 'Nov-20-2025_01-34-754bad', 'dataset': 'Video_Games', 'device': device(type='cuda'), 'use_ddp': False, 'accelerator': <accelerate.accelerator.Accelerator object at 0x73564abaf9a0>}
Thu 20 Nov 2025 01:34:49 INFO  [TOKENIZER] Loading semantic IDs from datasets/Video_Games/rqvae/sentence-t5-base_256,256,256,256_9976.sem_ids...
Thu 20 Nov 2025 01:34:49 INFO  [TOKENIZER] Loading semantic IDs from datasets/Video_Games/rqvae/sentence-t5-base_256,256,256,256_9977.sem_ids...
Thu 20 Nov 2025 01:34:49 INFO  [TOKENIZER] Loading semantic IDs from datasets/Video_Games/rqvae/sentence-t5-base_256,256,256,256_9978.sem_ids...
Thu 20 Nov 2025 01:34:50 INFO  [TOKENIZER] Loading semantic IDs from datasets/Video_Games/rqvae/sentence-t5-base_256,256,256,256_9979.sem_ids...
Thu 20 Nov 2025 01:34:50 INFO  [TOKENIZER] Loading semantic IDs from datasets/Video_Games/rqvae/sentence-t5-base_256,256,256,256_9980.sem_ids...
Thu 20 Nov 2025 01:34:50 INFO  [TOKENIZER] Loading semantic IDs from datasets/Video_Games/rqvae/sentence-t5-base_256,256,256,256_9981.sem_ids...
Thu 20 Nov 2025 01:34:50 INFO  [TOKENIZER] Loading semantic IDs from datasets/Video_Games/rqvae/sentence-t5-base_256,256,256,256_9982.sem_ids...
Thu 20 Nov 2025 01:34:50 INFO  [TOKENIZER] Loading semantic IDs from datasets/Video_Games/rqvae/sentence-t5-base_256,256,256,256_9983.sem_ids...
Thu 20 Nov 2025 01:34:50 INFO  [TOKENIZER] Loading semantic IDs from datasets/Video_Games/rqvae/sentence-t5-base_256,256,256,256_9984.sem_ids...
Thu 20 Nov 2025 01:34:50 INFO  [TOKENIZER] Loading semantic IDs from datasets/Video_Games/rqvae/sentence-t5-base_256,256,256,256_9985.sem_ids...
Thu 20 Nov 2025 01:34:51 INFO  [TOKENIZER] Loading semantic IDs from datasets/Video_Games/rqvae/sentence-t5-base_256,256,256,256_9986.sem_ids...
Thu 20 Nov 2025 01:34:51 INFO  [TOKENIZER] Loading semantic IDs from datasets/Video_Games/rqvae/sentence-t5-base_256,256,256,256_9987.sem_ids...
Thu 20 Nov 2025 01:34:51 INFO  [TOKENIZER] Loading semantic IDs from datasets/Video_Games/rqvae/sentence-t5-base_256,256,256,256_9988.sem_ids...
Thu 20 Nov 2025 01:34:51 INFO  [TOKENIZER] Loading semantic IDs from datasets/Video_Games/rqvae/sentence-t5-base_256,256,256,256_9989.sem_ids...
Thu 20 Nov 2025 01:34:51 INFO  [TOKENIZER] Loading semantic IDs from datasets/Video_Games/rqvae/sentence-t5-base_256,256,256,256_9990.sem_ids...
Thu 20 Nov 2025 01:34:51 INFO  [TOKENIZER] Loading semantic IDs from datasets/Video_Games/rqvae/sentence-t5-base_256,256,256,256_9991.sem_ids...
Thu 20 Nov 2025 01:34:51 INFO  [TOKENIZER] Loading semantic IDs from datasets/Video_Games/rqvae/sentence-t5-base_256,256,256,256_9992.sem_ids...
Thu 20 Nov 2025 01:34:51 INFO  [TOKENIZER] Loading semantic IDs from datasets/Video_Games/rqvae/sentence-t5-base_256,256,256,256_9993.sem_ids...
Thu 20 Nov 2025 01:34:52 INFO  [TOKENIZER] Loading semantic IDs from datasets/Video_Games/rqvae/sentence-t5-base_256,256,256,256_9994.sem_ids...
Thu 20 Nov 2025 01:34:52 INFO  [TOKENIZER] Loading semantic IDs from datasets/Video_Games/rqvae/sentence-t5-base_256,256,256,256_9995.sem_ids...
Thu 20 Nov 2025 01:34:52 INFO  [TOKENIZER] Loading semantic IDs from datasets/Video_Games/rqvae/sentence-t5-base_256,256,256,256_9996.sem_ids...
Thu 20 Nov 2025 01:34:52 INFO  [TOKENIZER] Loading semantic IDs from datasets/Video_Games/rqvae/sentence-t5-base_256,256,256,256_9997.sem_ids...
Thu 20 Nov 2025 01:34:52 INFO  [TOKENIZER] Loading semantic IDs from datasets/Video_Games/rqvae/sentence-t5-base_256,256,256,256_9998.sem_ids...
Thu 20 Nov 2025 01:34:52 INFO  [TOKENIZER] Loading semantic IDs from datasets/Video_Games/rqvae/sentence-t5-base_256,256,256,256_9999.sem_ids...
Thu 20 Nov 2025 01:34:52 INFO  [TOKENIZER] Loading semantic IDs from datasets/Video_Games/rqvae/sentence-t5-base_256,256,256,256_10000.sem_ids...
Thu 20 Nov 2025 01:34:57 INFO  MTGRec(
  (t5): T5ForConditionalGeneration(
    (shared): Embedding(1027, 128)
    (encoder): T5Stack(
      (embed_tokens): Embedding(1027, 128)
      (block): ModuleList(
        (0): T5Block(
          (layer): ModuleList(
            (0): T5LayerSelfAttention(
              (SelfAttention): T5Attention(
                (q): Linear(in_features=128, out_features=256, bias=False)
                (k): Linear(in_features=128, out_features=256, bias=False)
                (v): Linear(in_features=128, out_features=256, bias=False)
                (o): Linear(in_features=256, out_features=128, bias=False)
                (relative_attention_bias): Embedding(32, 4)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): T5LayerFF(
              (DenseReluDense): T5DenseActDense(
                (wi): Linear(in_features=128, out_features=512, bias=False)
                (wo): Linear(in_features=512, out_features=128, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (act): ReLU()
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1-6): 6 x T5Block(
          (layer): ModuleList(
            (0): T5LayerSelfAttention(
              (SelfAttention): T5Attention(
                (q): Linear(in_features=128, out_features=256, bias=False)
                (k): Linear(in_features=128, out_features=256, bias=False)
                (v): Linear(in_features=128, out_features=256, bias=False)
                (o): Linear(in_features=256, out_features=128, bias=False)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): T5LayerFF(
              (DenseReluDense): T5DenseActDense(
                (wi): Linear(in_features=128, out_features=512, bias=False)
                (wo): Linear(in_features=512, out_features=128, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (act): ReLU()
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (final_layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (decoder): T5Stack(
      (embed_tokens): Embedding(1027, 128)
      (block): ModuleList(
        (0): T5Block(
          (layer): ModuleList(
            (0): T5LayerSelfAttention(
              (SelfAttention): T5Attention(
                (q): Linear(in_features=128, out_features=256, bias=False)
                (k): Linear(in_features=128, out_features=256, bias=False)
                (v): Linear(in_features=128, out_features=256, bias=False)
                (o): Linear(in_features=256, out_features=128, bias=False)
                (relative_attention_bias): Embedding(32, 4)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): T5LayerCrossAttention(
              (EncDecAttention): T5Attention(
                (q): Linear(in_features=128, out_features=256, bias=False)
                (k): Linear(in_features=128, out_features=256, bias=False)
                (v): Linear(in_features=128, out_features=256, bias=False)
                (o): Linear(in_features=256, out_features=128, bias=False)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (2): T5LayerFF(
              (DenseReluDense): T5DenseActDense(
                (wi): Linear(in_features=128, out_features=512, bias=False)
                (wo): Linear(in_features=512, out_features=128, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (act): ReLU()
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1-6): 6 x T5Block(
          (layer): ModuleList(
            (0): T5LayerSelfAttention(
              (SelfAttention): T5Attention(
                (q): Linear(in_features=128, out_features=256, bias=False)
                (k): Linear(in_features=128, out_features=256, bias=False)
                (v): Linear(in_features=128, out_features=256, bias=False)
                (o): Linear(in_features=256, out_features=128, bias=False)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): T5LayerCrossAttention(
              (EncDecAttention): T5Attention(
                (q): Linear(in_features=128, out_features=256, bias=False)
                (k): Linear(in_features=128, out_features=256, bias=False)
                (v): Linear(in_features=128, out_features=256, bias=False)
                (o): Linear(in_features=256, out_features=128, bias=False)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (2): T5LayerFF(
              (DenseReluDense): T5DenseActDense(
                (wi): Linear(in_features=128, out_features=512, bias=False)
                (wo): Linear(in_features=512, out_features=128, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (act): ReLU()
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (final_layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (lm_head): Linear(in_features=128, out_features=1027, bias=False)
  )
)
Thu 20 Nov 2025 01:34:57 INFO  #Embedding parameters: 131456
#Non-embedding parameters: 4592512
#Total trainable parameters: 4723968

Thu 20 Nov 2025 01:37:21 INFO  [Epoch 1] Train Loss: 3.342456340444594
Thu 20 Nov 2025 01:39:43 INFO  [Epoch 2] Train Loss: 2.4881276167267536
Thu 20 Nov 2025 01:42:07 INFO  [Epoch 3] Train Loss: 2.3086121628873597
Thu 20 Nov 2025 01:44:33 INFO  [Epoch 4] Train Loss: 2.2243443770068034
Thu 20 Nov 2025 01:47:09 INFO  [Epoch 5] Train Loss: 2.1840662622543836
Thu 20 Nov 2025 01:49:41 INFO  [Epoch 6] Train Loss: 2.154827000781836
Thu 20 Nov 2025 01:52:19 INFO  [Epoch 7] Train Loss: 2.1297973002015853
Thu 20 Nov 2025 01:54:54 INFO  [Epoch 8] Train Loss: 2.103929345154394
Thu 20 Nov 2025 01:57:28 INFO  [Epoch 9] Train Loss: 2.086932207914393
Thu 20 Nov 2025 02:00:00 INFO  [Epoch 10] Train Loss: 2.074748675680529
Thu 20 Nov 2025 02:00:00 INFO  [Epoch 10] Saved model checkpoint to ckpt/Video_Games/Nov-20-2025_01-34-754bad/Nov-20-2025_01-34-754bad_10.pth
Thu 20 Nov 2025 02:02:34 INFO  [Epoch 11] Train Loss: 2.063753857288121
Thu 20 Nov 2025 02:05:07 INFO  [Epoch 12] Train Loss: 2.054828517743059
Thu 20 Nov 2025 02:07:42 INFO  [Epoch 13] Train Loss: 2.04848450606394
Thu 20 Nov 2025 02:10:17 INFO  [Epoch 14] Train Loss: 2.0404498421202297
Thu 20 Nov 2025 02:12:51 INFO  [Epoch 15] Train Loss: 2.0332544820534215
Thu 20 Nov 2025 02:15:21 INFO  [Epoch 16] Train Loss: 2.028941106784758
Thu 20 Nov 2025 02:17:52 INFO  [Epoch 17] Train Loss: 2.0271425483654824
Thu 20 Nov 2025 02:20:22 INFO  [Epoch 18] Train Loss: 2.0230475668166137
Thu 20 Nov 2025 02:22:56 INFO  [Epoch 19] Train Loss: 2.0174853557210173
Thu 20 Nov 2025 02:25:32 INFO  [Epoch 20] Train Loss: 2.0143065600436625
Thu 20 Nov 2025 02:25:32 INFO  [Epoch 20] Saved model checkpoint to ckpt/Video_Games/Nov-20-2025_01-34-754bad/Nov-20-2025_01-34-754bad_20.pth
Thu 20 Nov 2025 02:28:06 INFO  [Epoch 21] Train Loss: 2.012282103760362
Thu 20 Nov 2025 02:30:40 INFO  [Epoch 22] Train Loss: 2.009753776102913
Thu 20 Nov 2025 02:33:15 INFO  [Epoch 23] Train Loss: 2.007814961050468
Thu 20 Nov 2025 02:35:49 INFO  [Epoch 24] Train Loss: 2.006492736774522
Thu 20 Nov 2025 02:38:23 INFO  [Epoch 25] Train Loss: 2.0031613045332515
Thu 20 Nov 2025 02:40:57 INFO  [Epoch 26] Train Loss: 2.001728555007791
Thu 20 Nov 2025 02:43:32 INFO  [Epoch 27] Train Loss: 1.9995953957891832
Thu 20 Nov 2025 02:46:06 INFO  [Epoch 28] Train Loss: 1.9979833319035276
Thu 20 Nov 2025 02:48:40 INFO  [Epoch 29] Train Loss: 1.9980975982075033
Thu 20 Nov 2025 02:51:15 INFO  [Epoch 30] Train Loss: 1.9956639720896496
Thu 20 Nov 2025 02:51:15 INFO  [Epoch 30] Saved model checkpoint to ckpt/Video_Games/Nov-20-2025_01-34-754bad/Nov-20-2025_01-34-754bad_30.pth
Thu 20 Nov 2025 02:53:47 INFO  [Epoch 31] Train Loss: 1.994385339406006
Thu 20 Nov 2025 02:56:18 INFO  [Epoch 32] Train Loss: 1.9933984391247443
Thu 20 Nov 2025 02:58:49 INFO  [Epoch 33] Train Loss: 1.992086317893621
Thu 20 Nov 2025 03:01:22 INFO  [Epoch 34] Train Loss: 1.9901772515999305
Thu 20 Nov 2025 03:03:51 INFO  [Epoch 35] Train Loss: 1.9894997876813514
Thu 20 Nov 2025 03:06:20 INFO  [Epoch 36] Train Loss: 1.988452147850659
Thu 20 Nov 2025 03:08:53 INFO  [Epoch 37] Train Loss: 1.9858654538406828
Thu 20 Nov 2025 03:11:25 INFO  [Epoch 38] Train Loss: 1.9846794448188834
Thu 20 Nov 2025 03:13:59 INFO  [Epoch 39] Train Loss: 1.9853360349147016
Thu 20 Nov 2025 03:16:33 INFO  [Epoch 40] Train Loss: 1.9819720499073676
Thu 20 Nov 2025 03:16:33 INFO  [Epoch 40] Saved model checkpoint to ckpt/Video_Games/Nov-20-2025_01-34-754bad/Nov-20-2025_01-34-754bad_40.pth
Thu 20 Nov 2025 03:19:06 INFO  [Epoch 41] Train Loss: 1.9829497463339545
Thu 20 Nov 2025 03:21:40 INFO  [Epoch 42] Train Loss: 1.98044049745591
Thu 20 Nov 2025 03:24:14 INFO  [Epoch 43] Train Loss: 1.9794900096866614
Thu 20 Nov 2025 03:26:48 INFO  [Epoch 44] Train Loss: 1.9776848328504784
Thu 20 Nov 2025 03:29:21 INFO  [Epoch 45] Train Loss: 1.9774995646996847
Thu 20 Nov 2025 03:31:55 INFO  [Epoch 46] Train Loss: 1.9745546744481937
Thu 20 Nov 2025 03:34:30 INFO  [Epoch 47] Train Loss: 1.97478266071859
Thu 20 Nov 2025 03:37:04 INFO  [Epoch 48] Train Loss: 1.9730371431839513
Thu 20 Nov 2025 03:39:38 INFO  [Epoch 49] Train Loss: 1.9715626355418827
Thu 20 Nov 2025 03:42:12 INFO  [Epoch 50] Train Loss: 1.9706591343557514
Thu 20 Nov 2025 03:42:12 INFO  [Epoch 50] Saved model checkpoint to ckpt/Video_Games/Nov-20-2025_01-34-754bad/Nov-20-2025_01-34-754bad_50.pth
Thu 20 Nov 2025 03:44:45 INFO  [Epoch 51] Train Loss: 1.9691665831334804
Thu 20 Nov 2025 03:47:20 INFO  [Epoch 52] Train Loss: 1.968296141122759
Thu 20 Nov 2025 03:49:54 INFO  [Epoch 53] Train Loss: 1.9679987164768014
Thu 20 Nov 2025 03:52:29 INFO  [Epoch 54] Train Loss: 1.968722794909735
Thu 20 Nov 2025 03:55:02 INFO  [Epoch 55] Train Loss: 1.9643128545357913
Thu 20 Nov 2025 03:57:37 INFO  [Epoch 56] Train Loss: 1.9637864855150458
Thu 20 Nov 2025 04:00:09 INFO  [Epoch 57] Train Loss: 1.9615799937123957
Thu 20 Nov 2025 04:02:44 INFO  [Epoch 58] Train Loss: 1.9612804245074282
Thu 20 Nov 2025 04:05:18 INFO  [Epoch 59] Train Loss: 1.9595733870640686
Thu 20 Nov 2025 04:07:50 INFO  [Epoch 60] Train Loss: 1.9584490144229765
Thu 20 Nov 2025 04:07:50 INFO  [Epoch 60] Saved model checkpoint to ckpt/Video_Games/Nov-20-2025_01-34-754bad/Nov-20-2025_01-34-754bad_60.pth
Thu 20 Nov 2025 04:07:50 INFO  Best epoch: 0, Best val score: -1
Thu 20 Nov 2025 04:07:50 INFO  [Epoch 61] Saved model checkpoint to ckpt/Video_Games/Nov-20-2025_01-34-754bad/Nov-20-2025_01-34-754bad.pth
Thu 20 Nov 2025 04:07:50 INFO  Validation scores for all tokenizers: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
Thu 20 Nov 2025 05:23:12 INFO  Influence scores: tensor([[0.4431, 0.4224, 0.3983, 0.4282, 0.3812, 0.3795, 0.4131, 0.3723, 0.4021,
         0.4075, 0.3642, 0.3900, 0.3919, 0.3689, 0.4001, 0.3897, 0.3955, 0.3943,
         0.3765, 0.3602, 0.3714, 0.3813, 0.3620, 0.3924, 0.3693],
        [0.3972, 0.4683, 0.4117, 0.4052, 0.3816, 0.3855, 0.4122, 0.3803, 0.3923,
         0.3989, 0.3607, 0.3836, 0.3908, 0.3644, 0.4112, 0.3920, 0.3997, 0.3888,
         0.3720, 0.3653, 0.3779, 0.3789, 0.3684, 0.3936, 0.3724],
        [0.3954, 0.4335, 0.4274, 0.4026, 0.3866, 0.3822, 0.4173, 0.3813, 0.4034,
         0.4099, 0.3692, 0.3903, 0.3987, 0.3744, 0.4138, 0.4016, 0.4070, 0.3989,
         0.3804, 0.3716, 0.3830, 0.3863, 0.3697, 0.3961, 0.3730],
        [0.4095, 0.4119, 0.3869, 0.4410, 0.3854, 0.3781, 0.4189, 0.3774, 0.4037,
         0.4029, 0.3660, 0.3939, 0.3962, 0.3706, 0.4064, 0.3919, 0.3981, 0.3960,
         0.3820, 0.3603, 0.3710, 0.3803, 0.3625, 0.3922, 0.3686],
        [0.3760, 0.4031, 0.3852, 0.3988, 0.3967, 0.3797, 0.4169, 0.3768, 0.4031,
         0.4046, 0.3647, 0.3937, 0.3961, 0.3738, 0.4107, 0.3934, 0.3995, 0.3928,
         0.3786, 0.3610, 0.3712, 0.3775, 0.3588, 0.3840, 0.3605],
        [0.3724, 0.4039, 0.3783, 0.3904, 0.3770, 0.3941, 0.4141, 0.3722, 0.3959,
         0.4047, 0.3623, 0.3885, 0.3929, 0.3696, 0.4047, 0.3880, 0.3948, 0.3852,
         0.3693, 0.3548, 0.3659, 0.3719, 0.3570, 0.3829, 0.3587],
        [0.3656, 0.3902, 0.3729, 0.3895, 0.3741, 0.3732, 0.4300, 0.3673, 0.3997,
         0.4036, 0.3590, 0.3923, 0.3971, 0.3692, 0.4051, 0.3897, 0.3959, 0.3891,
         0.3751, 0.3532, 0.3650, 0.3737, 0.3516, 0.3818, 0.3562],
        [0.3627, 0.3967, 0.3756, 0.3864, 0.3726, 0.3700, 0.4054, 0.4180, 0.4025,
         0.4053, 0.3746, 0.3863, 0.4014, 0.3795, 0.4233, 0.4002, 0.4090, 0.4046,
         0.3806, 0.3749, 0.3840, 0.3872, 0.3789, 0.4037, 0.3773],
        [0.3597, 0.3752, 0.3641, 0.3802, 0.3661, 0.3626, 0.4063, 0.3709, 0.4281,
         0.4164, 0.3660, 0.3941, 0.4019, 0.3825, 0.4114, 0.3973, 0.4045, 0.4024,
         0.3800, 0.3631, 0.3739, 0.3827, 0.3605, 0.3859, 0.3604],
        [0.3680, 0.3849, 0.3737, 0.3834, 0.3705, 0.3736, 0.4137, 0.3756, 0.4182,
         0.4480, 0.3794, 0.4046, 0.4157, 0.3968, 0.4214, 0.4088, 0.4185, 0.4123,
         0.3894, 0.3735, 0.3876, 0.3962, 0.3749, 0.4009, 0.3747],
        [0.3543, 0.3769, 0.3634, 0.3749, 0.3600, 0.3610, 0.3984, 0.3750, 0.3980,
         0.4098, 0.3939, 0.3981, 0.4065, 0.3840, 0.4121, 0.4069, 0.4111, 0.4034,
         0.3890, 0.3734, 0.3812, 0.3874, 0.3757, 0.4023, 0.3753],
        [0.3457, 0.3654, 0.3499, 0.3686, 0.3548, 0.3517, 0.3968, 0.3529, 0.3927,
         0.3996, 0.3636, 0.4270, 0.3987, 0.3808, 0.4105, 0.4022, 0.4064, 0.4010,
         0.3871, 0.3661, 0.3765, 0.3911, 0.3642, 0.3936, 0.3700],
        [0.3450, 0.3695, 0.3553, 0.3687, 0.3541, 0.3530, 0.3985, 0.3651, 0.3962,
         0.4084, 0.3684, 0.3957, 0.4280, 0.3888, 0.4245, 0.4106, 0.4228, 0.4121,
         0.3927, 0.3774, 0.3907, 0.3979, 0.3766, 0.4057, 0.3767],
        [0.3458, 0.3670, 0.3548, 0.3669, 0.3552, 0.3532, 0.3945, 0.3664, 0.4017,
         0.4132, 0.3698, 0.4022, 0.4127, 0.4123, 0.4331, 0.4140, 0.4270, 0.4211,
         0.3946, 0.3844, 0.3980, 0.4064, 0.3841, 0.4093, 0.3833],
        [0.3307, 0.3668, 0.3468, 0.3554, 0.3454, 0.3417, 0.3829, 0.3632, 0.3829,
         0.3906, 0.3507, 0.3840, 0.4005, 0.3857, 0.4532, 0.4047, 0.4240, 0.4144,
         0.3874, 0.3837, 0.4023, 0.4036, 0.3863, 0.4098, 0.3858],
        [0.3439, 0.3726, 0.3604, 0.3659, 0.3536, 0.3512, 0.3937, 0.3653, 0.3952,
         0.4041, 0.3718, 0.4019, 0.4130, 0.3928, 0.4317, 0.4396, 0.4373, 0.4289,
         0.4068, 0.3981, 0.4073, 0.4142, 0.3934, 0.4204, 0.3918],
        [0.3335, 0.3630, 0.3487, 0.3552, 0.3424, 0.3405, 0.3824, 0.3572, 0.3849,
         0.3969, 0.3586, 0.3886, 0.4076, 0.3885, 0.4336, 0.4199, 0.4475, 0.4290,
         0.4019, 0.3962, 0.4130, 0.4178, 0.3977, 0.4234, 0.3959],
        [0.3302, 0.3507, 0.3395, 0.3519, 0.3355, 0.3306, 0.3757, 0.3522, 0.3815,
         0.3897, 0.3503, 0.3830, 0.3966, 0.3814, 0.4225, 0.4107, 0.4280, 0.4433,
         0.4028, 0.3927, 0.4047, 0.4187, 0.3908, 0.4216, 0.3923],
        [0.3336, 0.3549, 0.3416, 0.3590, 0.3411, 0.3344, 0.3811, 0.3494, 0.3790,
         0.3868, 0.3560, 0.3887, 0.3975, 0.3753, 0.4160, 0.4093, 0.4216, 0.4227,
         0.4369, 0.3955, 0.4059, 0.4183, 0.3962, 0.4290, 0.4009],
        [0.3331, 0.3642, 0.3492, 0.3537, 0.3402, 0.3367, 0.3762, 0.3586, 0.3796,
         0.3885, 0.3576, 0.3853, 0.3985, 0.3823, 0.4289, 0.4176, 0.4327, 0.4302,
         0.4128, 0.4278, 0.4316, 0.4322, 0.4181, 0.4404, 0.4141],
        [0.3245, 0.3558, 0.3397, 0.3444, 0.3301, 0.3276, 0.3675, 0.3472, 0.3705,
         0.3814, 0.3448, 0.3752, 0.3901, 0.3740, 0.4256, 0.4051, 0.4279, 0.4210,
         0.4023, 0.4107, 0.4494, 0.4388, 0.4282, 0.4469, 0.4263],
        [0.3170, 0.3412, 0.3270, 0.3365, 0.3195, 0.3162, 0.3597, 0.3345, 0.3622,
         0.3729, 0.3340, 0.3726, 0.3813, 0.3657, 0.4110, 0.3962, 0.4161, 0.4177,
         0.3972, 0.3944, 0.4231, 0.4563, 0.4164, 0.4491, 0.4265],
        [0.3228, 0.3548, 0.3348, 0.3434, 0.3245, 0.3248, 0.3609, 0.3504, 0.3629,
         0.3762, 0.3457, 0.3700, 0.3836, 0.3683, 0.4177, 0.3988, 0.4195, 0.4145,
         0.4000, 0.4045, 0.4362, 0.4408, 0.4478, 0.4609, 0.4391],
        [0.3241, 0.3504, 0.3318, 0.3435, 0.3202, 0.3211, 0.3618, 0.3443, 0.3593,
         0.3725, 0.3429, 0.3696, 0.3822, 0.3625, 0.4105, 0.3968, 0.4159, 0.4149,
         0.4023, 0.3972, 0.4266, 0.4443, 0.4310, 0.4820, 0.4497],
        [0.3105, 0.3400, 0.3194, 0.3300, 0.3079, 0.3081, 0.3468, 0.3296, 0.3445,
         0.3561, 0.3264, 0.3567, 0.3657, 0.3482, 0.3984, 0.3793, 0.3995, 0.3971,
         0.3852, 0.3819, 0.4149, 0.4323, 0.4198, 0.4594, 0.4522]],
       device='cuda:0')
Thu 20 Nov 2025 05:23:12 INFO  Mean influence score: [0.3902077376842499, 0.3901129961013794, 0.39414718747138977, 0.3900820016860962, 0.3862856924533844, 0.381977915763855, 0.38079962134361267, 0.39041945338249207, 0.3838520348072052, 0.3945714831352234, 0.3868861794471741, 0.3806661069393158, 0.3872935473918915, 0.3908325135707855, 0.38329795002937317, 0.39420509338378906, 0.3889581263065338, 0.38307154178619385, 0.3852359354496002, 0.3916082978248596, 0.3862025737762451, 0.3777691423892975, 0.38411185145378113, 0.3822946846485138, 0.3683992028236389]
Thu 20 Nov 2025 05:23:12 INFO  Stage 0 selected prob: [0.040054310113191605, 0.040053047239780426, 0.040106941014528275, 0.040052630007267, 0.04000198096036911, 0.03994458168745041, 0.039928894490003586, 0.04005713760852814, 0.0399695448577404, 0.04011261835694313, 0.040009986609220505, 0.03992712125182152, 0.04001542180776596, 0.04006265103816986, 0.03996216133236885, 0.040107715874910355, 0.04003763198852539, 0.039959147572517395, 0.03998798504471779, 0.04007301479578018, 0.04000087082386017, 0.03988857939839363, 0.03997300565242767, 0.039948802441358566, 0.03976419195532799]
Thu 20 Nov 2025 05:25:46 INFO  [Epoch 61] Train Loss: 1.9580125382393023
Thu 20 Nov 2025 05:28:21 INFO  [Epoch 62] Train Loss: 1.954898241864208
Thu 20 Nov 2025 05:30:55 INFO  [Epoch 63] Train Loss: 1.9540438794146173
Thu 20 Nov 2025 05:33:30 INFO  [Epoch 64] Train Loss: 1.954211588704448
Thu 20 Nov 2025 05:36:05 INFO  [Epoch 65] Train Loss: 1.9515851329882632
Thu 20 Nov 2025 05:38:39 INFO  [Epoch 66] Train Loss: 1.9504114530376486
Thu 20 Nov 2025 05:41:14 INFO  [Epoch 67] Train Loss: 1.9492474115493215
Thu 20 Nov 2025 05:43:50 INFO  [Epoch 68] Train Loss: 1.9474203119982163
Thu 20 Nov 2025 05:46:24 INFO  [Epoch 69] Train Loss: 1.9466656444730905
Thu 20 Nov 2025 05:48:59 INFO  [Epoch 70] Train Loss: 1.9436955217228893
Thu 20 Nov 2025 05:48:59 INFO  [Epoch 70] Saved model checkpoint to ckpt/Video_Games/Nov-20-2025_01-34-754bad/Nov-20-2025_01-34-754bad_70.pth
Thu 20 Nov 2025 05:51:33 INFO  [Epoch 71] Train Loss: 1.9439941973184527
Thu 20 Nov 2025 05:54:08 INFO  [Epoch 72] Train Loss: 1.9415555179234176
Thu 20 Nov 2025 05:56:41 INFO  [Epoch 73] Train Loss: 1.940861654730377
Thu 20 Nov 2025 05:59:16 INFO  [Epoch 74] Train Loss: 1.938477589989721
Thu 20 Nov 2025 06:01:51 INFO  [Epoch 75] Train Loss: 1.9383048616551064
Thu 20 Nov 2025 06:04:25 INFO  [Epoch 76] Train Loss: 1.9363294618355262
Thu 20 Nov 2025 06:06:59 INFO  [Epoch 77] Train Loss: 1.934545216010344
Thu 20 Nov 2025 06:09:35 INFO  [Epoch 78] Train Loss: 1.932692950411653
Thu 20 Nov 2025 06:12:11 INFO  [Epoch 79] Train Loss: 1.933667195522187
Thu 20 Nov 2025 06:14:47 INFO  [Epoch 80] Train Loss: 1.929470216963282
Thu 20 Nov 2025 06:14:47 INFO  [Epoch 80] Saved model checkpoint to ckpt/Video_Games/Nov-20-2025_01-34-754bad/Nov-20-2025_01-34-754bad_80.pth
Thu 20 Nov 2025 06:14:47 INFO  Best epoch: 0, Best val score: -1
Thu 20 Nov 2025 06:14:47 INFO  [Epoch 81] Saved model checkpoint to ckpt/Video_Games/Nov-20-2025_01-34-754bad/Nov-20-2025_01-34-754bad.pth
Thu 20 Nov 2025 06:14:47 INFO  Validation scores for all tokenizers: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
Thu 20 Nov 2025 07:29:55 INFO  Influence scores: tensor([[0.3070, 0.2321, 0.2172, 0.2411, 0.1917, 0.1953, 0.1966, 0.1743, 0.2028,
         0.1775, 0.1858, 0.1943, 0.1544, 0.1435, 0.1473, 0.1461, 0.1287, 0.1468,
         0.1481, 0.1321, 0.1179, 0.1375, 0.1454, 0.1712, 0.1570],
        [0.2503, 0.2738, 0.2241, 0.2094, 0.1865, 0.1948, 0.1904, 0.1760, 0.1851,
         0.1611, 0.1774, 0.1798, 0.1477, 0.1330, 0.1524, 0.1413, 0.1253, 0.1332,
         0.1364, 0.1300, 0.1157, 0.1264, 0.1426, 0.1641, 0.1530],
        [0.2447, 0.2335, 0.2386, 0.2035, 0.1881, 0.1879, 0.1922, 0.1740, 0.1944,
         0.1702, 0.1828, 0.1840, 0.1525, 0.1395, 0.1506, 0.1484, 0.1303, 0.1408,
         0.1425, 0.1338, 0.1180, 0.1304, 0.1407, 0.1629, 0.1497],
        [0.2532, 0.2039, 0.1882, 0.2377, 0.1795, 0.1776, 0.1871, 0.1662, 0.1889,
         0.1579, 0.1727, 0.1819, 0.1444, 0.1303, 0.1388, 0.1338, 0.1166, 0.1350,
         0.1391, 0.1178, 0.1022, 0.1204, 0.1290, 0.1555, 0.1401],
        [0.2160, 0.1933, 0.1854, 0.1913, 0.1919, 0.1790, 0.1850, 0.1655, 0.1874,
         0.1585, 0.1721, 0.1807, 0.1439, 0.1325, 0.1413, 0.1352, 0.1170, 0.1305,
         0.1341, 0.1173, 0.1005, 0.1164, 0.1227, 0.1456, 0.1314],
        [0.2175, 0.2002, 0.1837, 0.1875, 0.1774, 0.2018, 0.1872, 0.1657, 0.1862,
         0.1645, 0.1764, 0.1813, 0.1469, 0.1345, 0.1420, 0.1364, 0.1195, 0.1290,
         0.1303, 0.1168, 0.1021, 0.1174, 0.1280, 0.1513, 0.1365],
        [0.2013, 0.1780, 0.1704, 0.1785, 0.1646, 0.1689, 0.1970, 0.1518, 0.1817,
         0.1554, 0.1641, 0.1768, 0.1429, 0.1252, 0.1339, 0.1293, 0.1120, 0.1248,
         0.1288, 0.1073, 0.0929, 0.1113, 0.1137, 0.1420, 0.1251],
        [0.1809, 0.1660, 0.1548, 0.1602, 0.1473, 0.1485, 0.1537, 0.1904, 0.1681,
         0.1392, 0.1638, 0.1531, 0.1315, 0.1188, 0.1350, 0.1217, 0.1080, 0.1212,
         0.1173, 0.1109, 0.0934, 0.1053, 0.1253, 0.1457, 0.1286],
        [0.1947, 0.1597, 0.1602, 0.1688, 0.1553, 0.1556, 0.1699, 0.1549, 0.2123,
         0.1684, 0.1707, 0.1797, 0.1486, 0.1402, 0.1388, 0.1370, 0.1208, 0.1391,
         0.1341, 0.1171, 0.1019, 0.1209, 0.1233, 0.1469, 0.1296],
        [0.2004, 0.1678, 0.1665, 0.1689, 0.1570, 0.1647, 0.1748, 0.1568, 0.1990,
         0.2000, 0.1825, 0.1880, 0.1607, 0.1520, 0.1472, 0.1471, 0.1338, 0.1482,
         0.1415, 0.1249, 0.1131, 0.1319, 0.1353, 0.1600, 0.1417],
        [0.1845, 0.1596, 0.1553, 0.1598, 0.1467, 0.1529, 0.1594, 0.1577, 0.1775,
         0.1596, 0.2005, 0.1812, 0.1513, 0.1391, 0.1374, 0.1458, 0.1253, 0.1385,
         0.1408, 0.1257, 0.1064, 0.1213, 0.1372, 0.1617, 0.1429],
        [0.1844, 0.1529, 0.1482, 0.1599, 0.1471, 0.1500, 0.1646, 0.1381, 0.1790,
         0.1556, 0.1728, 0.2182, 0.1491, 0.1426, 0.1416, 0.1470, 0.1263, 0.1429,
         0.1469, 0.1237, 0.1067, 0.1327, 0.1284, 0.1571, 0.1415],
        [0.1791, 0.1554, 0.1508, 0.1564, 0.1437, 0.1483, 0.1631, 0.1504, 0.1803,
         0.1616, 0.1754, 0.1814, 0.1796, 0.1479, 0.1554, 0.1539, 0.1429, 0.1528,
         0.1492, 0.1335, 0.1208, 0.1372, 0.1401, 0.1673, 0.1483],
        [0.1800, 0.1514, 0.1494, 0.1544, 0.1440, 0.1473, 0.1571, 0.1495, 0.1847,
         0.1651, 0.1749, 0.1880, 0.1602, 0.1708, 0.1621, 0.1550, 0.1446, 0.1577,
         0.1492, 0.1383, 0.1258, 0.1435, 0.1460, 0.1682, 0.1522],
        [0.1683, 0.1547, 0.1435, 0.1473, 0.1377, 0.1390, 0.1499, 0.1494, 0.1679,
         0.1441, 0.1576, 0.1717, 0.1511, 0.1463, 0.1873, 0.1468, 0.1445, 0.1526,
         0.1450, 0.1405, 0.1329, 0.1433, 0.1518, 0.1711, 0.1578],
        [0.1788, 0.1565, 0.1543, 0.1544, 0.1428, 0.1458, 0.1576, 0.1482, 0.1770,
         0.1561, 0.1780, 0.1882, 0.1621, 0.1507, 0.1593, 0.1839, 0.1557, 0.1686,
         0.1635, 0.1537, 0.1364, 0.1530, 0.1566, 0.1822, 0.1627],
        [0.1690, 0.1479, 0.1437, 0.1453, 0.1328, 0.1367, 0.1476, 0.1432, 0.1689,
         0.1509, 0.1659, 0.1757, 0.1590, 0.1490, 0.1652, 0.1639, 0.1699, 0.1719,
         0.1604, 0.1539, 0.1449, 0.1585, 0.1632, 0.1870, 0.1683],
        [0.1702, 0.1396, 0.1383, 0.1465, 0.1293, 0.1304, 0.1444, 0.1396, 0.1696,
         0.1482, 0.1618, 0.1751, 0.1524, 0.1448, 0.1561, 0.1599, 0.1544, 0.1908,
         0.1674, 0.1538, 0.1398, 0.1635, 0.1602, 0.1892, 0.1680],
        [0.1665, 0.1372, 0.1345, 0.1454, 0.1279, 0.1264, 0.1428, 0.1308, 0.1591,
         0.1368, 0.1594, 0.1735, 0.1439, 0.1307, 0.1439, 0.1500, 0.1389, 0.1626,
         0.1966, 0.1514, 0.1341, 0.1557, 0.1593, 0.1897, 0.1695],
        [0.1751, 0.1552, 0.1499, 0.1498, 0.1363, 0.1370, 0.1459, 0.1488, 0.1685,
         0.1459, 0.1689, 0.1769, 0.1538, 0.1460, 0.1644, 0.1652, 0.1575, 0.1746,
         0.1765, 0.1916, 0.1682, 0.1768, 0.1899, 0.2076, 0.1899],
        [0.1697, 0.1506, 0.1434, 0.1438, 0.1288, 0.1314, 0.1412, 0.1404, 0.1628,
         0.1428, 0.1585, 0.1696, 0.1486, 0.1417, 0.1650, 0.1554, 0.1564, 0.1685,
         0.1675, 0.1764, 0.1902, 0.1871, 0.2035, 0.2185, 0.2059],
        [0.1677, 0.1400, 0.1349, 0.1403, 0.1224, 0.1249, 0.1376, 0.1313, 0.1598,
         0.1393, 0.1522, 0.1726, 0.1436, 0.1381, 0.1536, 0.1509, 0.1487, 0.1708,
         0.1678, 0.1636, 0.1663, 0.2111, 0.1951, 0.2259, 0.2118],
        [0.1643, 0.1453, 0.1329, 0.1376, 0.1182, 0.1241, 0.1284, 0.1395, 0.1510,
         0.1323, 0.1565, 0.1581, 0.1351, 0.1293, 0.1510, 0.1432, 0.1421, 0.1558,
         0.1595, 0.1649, 0.1708, 0.1835, 0.2195, 0.2278, 0.2162],
        [0.1708, 0.1467, 0.1355, 0.1435, 0.1194, 0.1263, 0.1353, 0.1386, 0.1520,
         0.1346, 0.1588, 0.1639, 0.1396, 0.1294, 0.1484, 0.1470, 0.1437, 0.1634,
         0.1686, 0.1622, 0.1658, 0.1930, 0.2071, 0.2580, 0.2329],
        [0.1657, 0.1443, 0.1311, 0.1382, 0.1158, 0.1218, 0.1288, 0.1317, 0.1463,
         0.1268, 0.1504, 0.1598, 0.1314, 0.1243, 0.1460, 0.1377, 0.1358, 0.1529,
         0.1588, 0.1549, 0.1625, 0.1890, 0.2047, 0.2424, 0.2456]],
       device='cuda:0')
Thu 20 Nov 2025 07:29:55 INFO  Mean influence score: [0.29293158650398254, 0.2895732820034027, 0.2922195792198181, 0.2857215702533722, 0.28140711784362793, 0.2798767387866974, 0.27483564615249634, 0.2766689360141754, 0.27778545022010803, 0.28755271434783936, 0.27980074286460876, 0.2762092649936676, 0.2819540798664093, 0.2847028374671936, 0.2784568667411804, 0.2884792387485504, 0.2841065526008606, 0.27998775243759155, 0.27887219190597534, 0.2887675166130066, 0.28486040234565735, 0.2784821689128876, 0.28043970465660095, 0.2812179625034332, 0.2711203396320343]
Thu 20 Nov 2025 07:29:55 INFO  Stage 1 selected prob: [0.0401427261531353, 0.04009781405329704, 0.04013320058584213, 0.040046364068984985, 0.03998881205916405, 0.039968423545360565, 0.03990131616592407, 0.0399257056415081, 0.039940569549798965, 0.04007081314921379, 0.039967406541109085, 0.03991958871483803, 0.039996106177568436, 0.0400327667593956, 0.039949510246515274, 0.04008319228887558, 0.04002481698989868, 0.039969898760318756, 0.03995504230260849, 0.04008704423904419, 0.04003487527370453, 0.039949845522642136, 0.03997591882944107, 0.03998629376292229, 0.03985193371772766]
Thu 20 Nov 2025 07:32:28 INFO  [Epoch 81] Train Loss: 1.9281628026580258
Thu 20 Nov 2025 07:35:03 INFO  [Epoch 82] Train Loss: 1.9269321575243041
Thu 20 Nov 2025 07:37:38 INFO  [Epoch 83] Train Loss: 1.9274288284157233
Thu 20 Nov 2025 07:40:13 INFO  [Epoch 84] Train Loss: 1.9244533886550477
Thu 20 Nov 2025 07:42:46 INFO  [Epoch 85] Train Loss: 1.9215484344936247
Thu 20 Nov 2025 07:45:21 INFO  [Epoch 86] Train Loss: 1.9213078161809436
Thu 20 Nov 2025 07:47:56 INFO  [Epoch 87] Train Loss: 1.9185394048690796
Thu 20 Nov 2025 07:50:30 INFO  [Epoch 88] Train Loss: 1.9166877124185266
Thu 20 Nov 2025 07:53:04 INFO  [Epoch 89] Train Loss: 1.9147106723205463
Thu 20 Nov 2025 07:55:36 INFO  [Epoch 90] Train Loss: 1.9130205595470304
Thu 20 Nov 2025 07:55:36 INFO  [Epoch 90] Saved model checkpoint to ckpt/Video_Games/Nov-20-2025_01-34-754bad/Nov-20-2025_01-34-754bad_90.pth
Thu 20 Nov 2025 07:58:10 INFO  [Epoch 91] Train Loss: 1.9117436307162392
Thu 20 Nov 2025 08:00:46 INFO  [Epoch 92] Train Loss: 1.9105062536521307
Thu 20 Nov 2025 08:03:21 INFO  [Epoch 93] Train Loss: 1.9095768952691876
Thu 20 Nov 2025 08:05:56 INFO  [Epoch 94] Train Loss: 1.9060631711970886
Thu 20 Nov 2025 08:08:30 INFO  [Epoch 95] Train Loss: 1.9045201257504092
Thu 20 Nov 2025 08:11:03 INFO  [Epoch 96] Train Loss: 1.90292453863676
Thu 20 Nov 2025 08:13:37 INFO  [Epoch 97] Train Loss: 1.9008144775988052
Thu 20 Nov 2025 08:16:09 INFO  [Epoch 98] Train Loss: 1.9008809782593408
Thu 20 Nov 2025 08:18:41 INFO  [Epoch 99] Train Loss: 1.8971427798156113
Thu 20 Nov 2025 08:21:14 INFO  [Epoch 100] Train Loss: 1.896253567059528
Thu 20 Nov 2025 08:21:15 INFO  [Epoch 100] Saved model checkpoint to ckpt/Video_Games/Nov-20-2025_01-34-754bad/Nov-20-2025_01-34-754bad_100.pth
Thu 20 Nov 2025 08:21:15 INFO  Best epoch: 0, Best val score: -1
Thu 20 Nov 2025 08:21:15 INFO  [Epoch 101] Saved model checkpoint to ckpt/Video_Games/Nov-20-2025_01-34-754bad/Nov-20-2025_01-34-754bad.pth
Thu 20 Nov 2025 08:21:15 INFO  Validation scores for all tokenizers: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
Thu 20 Nov 2025 09:38:41 INFO  Influence scores: tensor([[0.2550, 0.2326, 0.1923, 0.2003, 0.1564, 0.1817, 0.1294, 0.1403, 0.1211,
         0.1566, 0.1556, 0.1265, 0.1080, 0.1147, 0.1399, 0.1278, 0.1227, 0.1071,
         0.1431, 0.1511, 0.1442, 0.1043, 0.1331, 0.1104, 0.1092],
        [0.1964, 0.2871, 0.2067, 0.1694, 0.1564, 0.1887, 0.1274, 0.1470, 0.1074,
         0.1451, 0.1519, 0.1163, 0.1056, 0.1088, 0.1476, 0.1255, 0.1226, 0.0949,
         0.1364, 0.1523, 0.1458, 0.0955, 0.1347, 0.1055, 0.1076],
        [0.1891, 0.2383, 0.2236, 0.1619, 0.1601, 0.1812, 0.1309, 0.1453, 0.1214,
         0.1577, 0.1593, 0.1230, 0.1136, 0.1200, 0.1493, 0.1365, 0.1308, 0.1067,
         0.1444, 0.1592, 0.1514, 0.1032, 0.1342, 0.1060, 0.1058],
        [0.2083, 0.2136, 0.1740, 0.2102, 0.1566, 0.1768, 0.1312, 0.1433, 0.1192,
         0.1475, 0.1539, 0.1256, 0.1089, 0.1134, 0.1426, 0.1264, 0.1218, 0.1059,
         0.1455, 0.1478, 0.1405, 0.0990, 0.1303, 0.1066, 0.1055],
        [0.1673, 0.2035, 0.1751, 0.1588, 0.1743, 0.1812, 0.1330, 0.1463, 0.1228,
         0.1536, 0.1562, 0.1289, 0.1129, 0.1211, 0.1503, 0.1324, 0.1271, 0.1054,
         0.1440, 0.1523, 0.1438, 0.0989, 0.1272, 0.0984, 0.0983],
        [0.1670, 0.2094, 0.1696, 0.1528, 0.1547, 0.2049, 0.1324, 0.1449, 0.1186,
         0.1579, 0.1594, 0.1257, 0.1127, 0.1200, 0.1478, 0.1302, 0.1266, 0.1005,
         0.1357, 0.1480, 0.1421, 0.0960, 0.1301, 0.1012, 0.1006],
        [0.1548, 0.1887, 0.1607, 0.1475, 0.1465, 0.1726, 0.1496, 0.1341, 0.1190,
         0.1528, 0.1492, 0.1271, 0.1144, 0.1153, 0.1451, 0.1278, 0.1238, 0.1020,
         0.1399, 0.1436, 0.1383, 0.0953, 0.1193, 0.0971, 0.0937],
        [0.1464, 0.1901, 0.1567, 0.1407, 0.1414, 0.1672, 0.1162, 0.1888, 0.1202,
         0.1518, 0.1671, 0.1165, 0.1175, 0.1252, 0.1586, 0.1335, 0.1338, 0.1123,
         0.1436, 0.1617, 0.1521, 0.1035, 0.1468, 0.1153, 0.1123],
        [0.1491, 0.1709, 0.1522, 0.1381, 0.1384, 0.1616, 0.1218, 0.1411, 0.1602,
         0.1741, 0.1608, 0.1338, 0.1248, 0.1374, 0.1552, 0.1411, 0.1388, 0.1225,
         0.1482, 0.1586, 0.1526, 0.1102, 0.1333, 0.1045, 0.1014],
        [0.1500, 0.1732, 0.1535, 0.1319, 0.1351, 0.1663, 0.1215, 0.1375, 0.1393,
         0.2017, 0.1667, 0.1367, 0.1308, 0.1439, 0.1582, 0.1447, 0.1458, 0.1255,
         0.1495, 0.1607, 0.1577, 0.1151, 0.1396, 0.1107, 0.1072],
        [0.1376, 0.1703, 0.1461, 0.1275, 0.1284, 0.1579, 0.1080, 0.1439, 0.1163,
         0.1583, 0.1917, 0.1327, 0.1239, 0.1333, 0.1514, 0.1473, 0.1416, 0.1185,
         0.1545, 0.1670, 0.1556, 0.1099, 0.1485, 0.1200, 0.1158],
        [0.1332, 0.1590, 0.1349, 0.1233, 0.1258, 0.1497, 0.1112, 0.1174, 0.1151,
         0.1518, 0.1568, 0.1703, 0.1196, 0.1341, 0.1529, 0.1463, 0.1391, 0.1197,
         0.1568, 0.1614, 0.1530, 0.1181, 0.1349, 0.1120, 0.1107],
        [0.1263, 0.1595, 0.1359, 0.1183, 0.1194, 0.1463, 0.1078, 0.1289, 0.1151,
         0.1564, 0.1584, 0.1288, 0.1504, 0.1379, 0.1655, 0.1515, 0.1547, 0.1293,
         0.1591, 0.1712, 0.1661, 0.1212, 0.1457, 0.1208, 0.1154],
        [0.1244, 0.1539, 0.1332, 0.1133, 0.1193, 0.1453, 0.1006, 0.1282, 0.1204,
         0.1617, 0.1591, 0.1353, 0.1293, 0.1634, 0.1724, 0.1529, 0.1570, 0.1358,
         0.1571, 0.1753, 0.1706, 0.1275, 0.1510, 0.1207, 0.1183],
        [0.1243, 0.1671, 0.1363, 0.1176, 0.1229, 0.1472, 0.1041, 0.1364, 0.1127,
         0.1499, 0.1521, 0.1285, 0.1307, 0.1464, 0.2114, 0.1557, 0.1677, 0.1414,
         0.1642, 0.1884, 0.1893, 0.1375, 0.1677, 0.1353, 0.1353],
        [0.1206, 0.1536, 0.1337, 0.1105, 0.1152, 0.1394, 0.0970, 0.1204, 0.1080,
         0.1458, 0.1575, 0.1316, 0.1267, 0.1367, 0.1657, 0.1809, 0.1650, 0.1430,
         0.1710, 0.1891, 0.1779, 0.1346, 0.1593, 0.1331, 0.1274],
        [0.1176, 0.1532, 0.1295, 0.1079, 0.1114, 0.1371, 0.0939, 0.1221, 0.1066,
         0.1484, 0.1533, 0.1261, 0.1316, 0.1422, 0.1796, 0.1671, 0.1885, 0.1540,
         0.1752, 0.1973, 0.1951, 0.1487, 0.1747, 0.1460, 0.1412],
        [0.1213, 0.1457, 0.1259, 0.1116, 0.1098, 0.1319, 0.0928, 0.1213, 0.1108,
         0.1484, 0.1505, 0.1272, 0.1263, 0.1412, 0.1729, 0.1644, 0.1736, 0.1780,
         0.1846, 0.1998, 0.1929, 0.1572, 0.1738, 0.1517, 0.1445],
        [0.1221, 0.1513, 0.1281, 0.1159, 0.1131, 0.1314, 0.0953, 0.1178, 0.1014,
         0.1379, 0.1516, 0.1292, 0.1215, 0.1287, 0.1626, 0.1584, 0.1615, 0.1510,
         0.2220, 0.2019, 0.1914, 0.1542, 0.1778, 0.1590, 0.1523],
        [0.1192, 0.1569, 0.1327, 0.1083, 0.1120, 0.1343, 0.0885, 0.1259, 0.1020,
         0.1402, 0.1543, 0.1240, 0.1225, 0.1366, 0.1754, 0.1668, 0.1728, 0.1549,
         0.1907, 0.2370, 0.2190, 0.1677, 0.2017, 0.1681, 0.1646],
        [0.1148, 0.1531, 0.1264, 0.1031, 0.1050, 0.1282, 0.0842, 0.1172, 0.0961,
         0.1369, 0.1436, 0.1169, 0.1186, 0.1321, 0.1766, 0.1566, 0.1719, 0.1499,
         0.1826, 0.2207, 0.2426, 0.1781, 0.2161, 0.1794, 0.1801],
        [0.1097, 0.1378, 0.1139, 0.0967, 0.0953, 0.1178, 0.0774, 0.1039, 0.0898,
         0.1297, 0.1329, 0.1171, 0.1096, 0.1242, 0.1612, 0.1482, 0.1606, 0.1491,
         0.1795, 0.2040, 0.2131, 0.2027, 0.2043, 0.1842, 0.1844],
        [0.1127, 0.1516, 0.1181, 0.1008, 0.0967, 0.1248, 0.0737, 0.1207, 0.0858,
         0.1282, 0.1451, 0.1076, 0.1069, 0.1215, 0.1647, 0.1467, 0.1601, 0.1393,
         0.1774, 0.2120, 0.2247, 0.1782, 0.2387, 0.1935, 0.1963],
        [0.1153, 0.1476, 0.1161, 0.1036, 0.0934, 0.1219, 0.0772, 0.1147, 0.0825,
         0.1244, 0.1418, 0.1098, 0.1072, 0.1166, 0.1577, 0.1458, 0.1570, 0.1426,
         0.1833, 0.2043, 0.2137, 0.1835, 0.2190, 0.2232, 0.2108],
        [0.1124, 0.1475, 0.1136, 0.1004, 0.0912, 0.1190, 0.0723, 0.1094, 0.0779,
         0.1187, 0.1354, 0.1074, 0.1008, 0.1118, 0.1562, 0.1382, 0.1502, 0.1336,
         0.1754, 0.1980, 0.2119, 0.1812, 0.2185, 0.2079, 0.2267]],
       device='cuda:0')
Thu 20 Nov 2025 09:38:41 INFO  Mean influence score: [0.25512629747390747, 0.25179746747016907, 0.25448569655418396, 0.24865153431892395, 0.2450244426727295, 0.2436380833387375, 0.23855258524417877, 0.24157561361789703, 0.24251660704612732, 0.2505100965499878, 0.24376212060451508, 0.24007487297058105, 0.2451886385679245, 0.24760256707668304, 0.24445833265781403, 0.2505863606929779, 0.24842624366283417, 0.24546858668327332, 0.24443010985851288, 0.2532035708427429, 0.2498370110988617, 0.24320827424526215, 0.24547401070594788, 0.2459167242050171, 0.23742058873176575]
Thu 20 Nov 2025 09:38:41 INFO  Stage 2 selected prob: [0.04011811688542366, 0.04007362201809883, 0.0401095449924469, 0.04003162309527397, 0.039983246475458145, 0.03996478021144867, 0.03989708796143532, 0.03993731364607811, 0.03994984179735184, 0.04005642607808113, 0.03996643051505089, 0.03991733491420746, 0.03998543694615364, 0.04001762717962265, 0.03997570276260376, 0.04005745053291321, 0.040028613060712814, 0.0399891659617424, 0.039975330233573914, 0.040092404931783676, 0.04004744440317154, 0.039959050714969635, 0.03998924046754837, 0.03999514505267143, 0.03988203778862953]
Thu 20 Nov 2025 09:41:12 INFO  [Epoch 101] Train Loss: 1.8933706489539055
Thu 20 Nov 2025 09:43:46 INFO  [Epoch 102] Train Loss: 1.8917606477571731
Thu 20 Nov 2025 09:46:19 INFO  [Epoch 103] Train Loss: 1.8909278522357056
Thu 20 Nov 2025 09:48:52 INFO  [Epoch 104] Train Loss: 1.8889502256418287
Thu 20 Nov 2025 09:51:26 INFO  [Epoch 105] Train Loss: 1.8867290101111165
Thu 20 Nov 2025 09:53:59 INFO  [Epoch 106] Train Loss: 1.8841657773178055
Thu 20 Nov 2025 09:56:33 INFO  [Epoch 107] Train Loss: 1.8824876316150643
Thu 20 Nov 2025 09:59:10 INFO  [Epoch 108] Train Loss: 1.880263873451465
Thu 20 Nov 2025 10:01:52 INFO  [Epoch 109] Train Loss: 1.8777850731114163
Thu 20 Nov 2025 10:04:33 INFO  [Epoch 110] Train Loss: 1.8757347229825023
Thu 20 Nov 2025 10:04:33 INFO  [Epoch 110] Saved model checkpoint to ckpt/Video_Games/Nov-20-2025_01-34-754bad/Nov-20-2025_01-34-754bad_110.pth
Thu 20 Nov 2025 10:07:16 INFO  [Epoch 111] Train Loss: 1.873384044207201
Thu 20 Nov 2025 10:09:58 INFO  [Epoch 112] Train Loss: 1.8718333930697681
Thu 20 Nov 2025 10:12:39 INFO  [Epoch 113] Train Loss: 1.8691142069672064
Thu 20 Nov 2025 10:15:20 INFO  [Epoch 114] Train Loss: 1.8679574628479232
Thu 20 Nov 2025 10:18:01 INFO  [Epoch 115] Train Loss: 1.8651652821702847
Thu 20 Nov 2025 10:20:38 INFO  [Epoch 116] Train Loss: 1.8632667626768467
Thu 20 Nov 2025 10:23:19 INFO  [Epoch 117] Train Loss: 1.8611928335027805
Thu 20 Nov 2025 10:25:59 INFO  [Epoch 118] Train Loss: 1.8588423453933023
Thu 20 Nov 2025 10:28:40 INFO  [Epoch 119] Train Loss: 1.8570500414919209
Thu 20 Nov 2025 10:31:20 INFO  [Epoch 120] Train Loss: 1.8546898449019575
Thu 20 Nov 2025 10:31:20 INFO  [Epoch 120] Saved model checkpoint to ckpt/Video_Games/Nov-20-2025_01-34-754bad/Nov-20-2025_01-34-754bad_120.pth
Thu 20 Nov 2025 10:31:20 INFO  Best epoch: 0, Best val score: -1
Thu 20 Nov 2025 10:31:21 INFO  [Epoch 121] Saved model checkpoint to ckpt/Video_Games/Nov-20-2025_01-34-754bad/Nov-20-2025_01-34-754bad.pth
Thu 20 Nov 2025 10:31:21 INFO  Validation scores for all tokenizers: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
Thu 20 Nov 2025 11:53:00 INFO  Influence scores: tensor([[0.1990, 0.1307, 0.1298, 0.1509, 0.1137, 0.1333, 0.1112, 0.1000, 0.1247,
         0.1609, 0.1131, 0.0939, 0.0691, 0.1059, 0.0787, 0.0908, 0.0799, 0.0766,
         0.0367, 0.0570, 0.0571, 0.0555, 0.0269, 0.0459, 0.0173],
        [0.1307, 0.2033, 0.1524, 0.1192, 0.1200, 0.1476, 0.1163, 0.1120, 0.1137,
         0.1526, 0.1156, 0.0910, 0.0750, 0.1070, 0.0971, 0.0945, 0.0885, 0.0674,
         0.0348, 0.0649, 0.0666, 0.0503, 0.0346, 0.0471, 0.0214],
        [0.1270, 0.1487, 0.1735, 0.1131, 0.1242, 0.1401, 0.1202, 0.1113, 0.1272,
         0.1664, 0.1240, 0.0968, 0.0811, 0.1165, 0.0952, 0.1050, 0.0952, 0.0799,
         0.0440, 0.0715, 0.0708, 0.0593, 0.0333, 0.0481, 0.0202],
        [0.1506, 0.1190, 0.1166, 0.1751, 0.1246, 0.1356, 0.1238, 0.1136, 0.1320,
         0.1586, 0.1218, 0.1048, 0.0807, 0.1143, 0.0919, 0.1003, 0.0892, 0.0856,
         0.0511, 0.0636, 0.0623, 0.0596, 0.0312, 0.0507, 0.0224],
        [0.1100, 0.1165, 0.1242, 0.1202, 0.1524, 0.1489, 0.1315, 0.1252, 0.1409,
         0.1712, 0.1317, 0.1151, 0.0921, 0.1291, 0.1091, 0.1127, 0.1026, 0.0897,
         0.0551, 0.0740, 0.0725, 0.0652, 0.0356, 0.0489, 0.0217],
        [0.1078, 0.1226, 0.1182, 0.1083, 0.1268, 0.1763, 0.1293, 0.1180, 0.1328,
         0.1745, 0.1319, 0.1105, 0.0902, 0.1263, 0.1040, 0.1086, 0.1004, 0.0817,
         0.0433, 0.0680, 0.0696, 0.0612, 0.0367, 0.0504, 0.0228],
        [0.1028, 0.1080, 0.1148, 0.1136, 0.1262, 0.1465, 0.1598, 0.1168, 0.1426,
         0.1768, 0.1307, 0.1212, 0.1014, 0.1295, 0.1092, 0.1151, 0.1048, 0.0928,
         0.0578, 0.0711, 0.0730, 0.0686, 0.0325, 0.0545, 0.0244],
        [0.0850, 0.0979, 0.1003, 0.0972, 0.1141, 0.1294, 0.1109, 0.1751, 0.1369,
         0.1674, 0.1402, 0.0989, 0.0955, 0.1322, 0.1173, 0.1119, 0.1082, 0.0963,
         0.0534, 0.0830, 0.0803, 0.0672, 0.0543, 0.0633, 0.0333],
        [0.0896, 0.0781, 0.0955, 0.0952, 0.1092, 0.1235, 0.1165, 0.1164, 0.1818,
         0.1914, 0.1338, 0.1172, 0.1011, 0.1450, 0.1114, 0.1199, 0.1119, 0.1072,
         0.0580, 0.0784, 0.0795, 0.0754, 0.0402, 0.0534, 0.0234],
        [0.0858, 0.0783, 0.0939, 0.0823, 0.1004, 0.1248, 0.1119, 0.1073, 0.1501,
         0.2220, 0.1386, 0.1160, 0.1057, 0.1483, 0.1095, 0.1196, 0.1159, 0.1045,
         0.0545, 0.0773, 0.0816, 0.0770, 0.0437, 0.0575, 0.0261],
        [0.0763, 0.0786, 0.0908, 0.0836, 0.0997, 0.1211, 0.1040, 0.1193, 0.1332,
         0.1788, 0.1741, 0.1224, 0.1063, 0.1442, 0.1100, 0.1310, 0.1187, 0.1052,
         0.0686, 0.0909, 0.0853, 0.0785, 0.0581, 0.0727, 0.0400],
        [0.0772, 0.0733, 0.0831, 0.0867, 0.1022, 0.1191, 0.1136, 0.0971, 0.1367,
         0.1743, 0.1403, 0.1740, 0.1067, 0.1509, 0.1197, 0.1370, 0.1228, 0.1126,
         0.0777, 0.0915, 0.0890, 0.0942, 0.0512, 0.0718, 0.0441],
        [0.0590, 0.0636, 0.0737, 0.0683, 0.0843, 0.1039, 0.0983, 0.0989, 0.1255,
         0.1697, 0.1301, 0.1118, 0.1334, 0.1444, 0.1218, 0.1307, 0.1298, 0.1116,
         0.0677, 0.0904, 0.0939, 0.0862, 0.0518, 0.0711, 0.0383],
        [0.0567, 0.0559, 0.0698, 0.0630, 0.0831, 0.1013, 0.0887, 0.0979, 0.1309,
         0.1744, 0.1298, 0.1183, 0.1062, 0.1739, 0.1295, 0.1319, 0.1323, 0.1195,
         0.0661, 0.0947, 0.0977, 0.0923, 0.0578, 0.0695, 0.0402],
        [0.0429, 0.0582, 0.0607, 0.0546, 0.0755, 0.0915, 0.0809, 0.0952, 0.1100,
         0.1472, 0.1083, 0.0985, 0.0957, 0.1423, 0.1649, 0.1230, 0.1330, 0.1145,
         0.0631, 0.0986, 0.1098, 0.0943, 0.0669, 0.0755, 0.0500],
        [0.0529, 0.0565, 0.0709, 0.0615, 0.0788, 0.0963, 0.0870, 0.0891, 0.1173,
         0.1570, 0.1285, 0.1159, 0.1042, 0.1435, 0.1226, 0.1669, 0.1424, 0.1292,
         0.0839, 0.1122, 0.1089, 0.1036, 0.0698, 0.0864, 0.0518],
        [0.0423, 0.0484, 0.0590, 0.0504, 0.0673, 0.0858, 0.0752, 0.0839, 0.1084,
         0.1520, 0.1155, 0.1009, 0.1017, 0.1431, 0.1318, 0.1417, 0.1622, 0.1333,
         0.0795, 0.1136, 0.1216, 0.1109, 0.0797, 0.0931, 0.0606],
        [0.0477, 0.0376, 0.0537, 0.0553, 0.0648, 0.0782, 0.0732, 0.0825, 0.1132,
         0.1506, 0.1117, 0.1002, 0.0930, 0.1396, 0.1224, 0.1374, 0.1419, 0.1625,
         0.0899, 0.1153, 0.1170, 0.1199, 0.0777, 0.0975, 0.0627],
        [0.0472, 0.0434, 0.0567, 0.0608, 0.0689, 0.0790, 0.0777, 0.0793, 0.1048,
         0.1402, 0.1143, 0.1044, 0.0889, 0.1263, 0.1116, 0.1324, 0.1285, 0.1311,
         0.1382, 0.1205, 0.1171, 0.1178, 0.0844, 0.1072, 0.0737],
        [0.0352, 0.0424, 0.0526, 0.0414, 0.0566, 0.0720, 0.0593, 0.0774, 0.0928,
         0.1319, 0.1056, 0.0875, 0.0800, 0.1240, 0.1154, 0.1292, 0.1313, 0.1242,
         0.0880, 0.1510, 0.1397, 0.1224, 0.1023, 0.1088, 0.0775],
        [0.0297, 0.0380, 0.0457, 0.0353, 0.0491, 0.0665, 0.0549, 0.0684, 0.0877,
         0.1296, 0.0939, 0.0796, 0.0764, 0.1202, 0.1185, 0.1186, 0.1315, 0.1191,
         0.0790, 0.1326, 0.1721, 0.1368, 0.1218, 0.1251, 0.0990],
        [0.0375, 0.0320, 0.0434, 0.0409, 0.0496, 0.0663, 0.0586, 0.0639, 0.0916,
         0.1324, 0.0939, 0.0917, 0.0774, 0.1226, 0.1120, 0.1215, 0.1299, 0.1303,
         0.0881, 0.1245, 0.1462, 0.1794, 0.1182, 0.1415, 0.1150],
        [0.0311, 0.0385, 0.0402, 0.0355, 0.0434, 0.0651, 0.0463, 0.0740, 0.0804,
         0.1240, 0.0982, 0.0735, 0.0668, 0.1122, 0.1078, 0.1119, 0.1219, 0.1112,
         0.0778, 0.1269, 0.1535, 0.1408, 0.1519, 0.1438, 0.1199],
        [0.0348, 0.0351, 0.0393, 0.0395, 0.0404, 0.0624, 0.0508, 0.0672, 0.0769,
         0.1202, 0.0957, 0.0761, 0.0686, 0.1072, 0.1002, 0.1118, 0.1181, 0.1154,
         0.0846, 0.1180, 0.1414, 0.1480, 0.1284, 0.1794, 0.1373],
        [0.0327, 0.0365, 0.0388, 0.0387, 0.0408, 0.0624, 0.0484, 0.0639, 0.0744,
         0.1164, 0.0914, 0.0769, 0.0640, 0.1056, 0.1020, 0.1056, 0.1139, 0.1081,
         0.0781, 0.1138, 0.1424, 0.1499, 0.1309, 0.1643, 0.1602]],
       device='cuda:0')
Thu 20 Nov 2025 11:53:00 INFO  Mean influence score: [0.23069751262664795, 0.2282712161540985, 0.23097068071365356, 0.22593887150287628, 0.22357669472694397, 0.22193633019924164, 0.2180757075548172, 0.2203664779663086, 0.22118476033210754, 0.2278391569852829, 0.22247500717639923, 0.21968387067317963, 0.2228744626045227, 0.22506274282932281, 0.22163031995296478, 0.22793197631835938, 0.22564338147640228, 0.22303535044193268, 0.2222072333097458, 0.22900547087192535, 0.22603309154510498, 0.2208932787179947, 0.22213619947433472, 0.2225130796432495, 0.21508380770683289]
Thu 20 Nov 2025 11:53:00 INFO  Stage 3 selected prob: [0.04009200260043144, 0.040059592574834824, 0.040095653384923935, 0.04002846032381058, 0.039996955543756485, 0.039975088089704514, 0.03992368280887604, 0.03995417803525925, 0.039965078234672546, 0.040053825825452805, 0.03998227044939995, 0.039945088326931, 0.03998759016394615, 0.0400167740881443, 0.039971012622117996, 0.040055062621831894, 0.040024518966674805, 0.039989735931158066, 0.03997870162129402, 0.04006939381361008, 0.04002971947193146, 0.03996119275689125, 0.03997775912284851, 0.03998277708888054, 0.03988388553261757]
Thu 20 Nov 2025 11:55:40 INFO  [Epoch 121] Train Loss: 1.8509882425594515
Thu 20 Nov 2025 11:58:16 INFO  [Epoch 122] Train Loss: 1.849650307108997
Thu 20 Nov 2025 12:00:57 INFO  [Epoch 123] Train Loss: 1.8473315988038037
Thu 20 Nov 2025 12:03:38 INFO  [Epoch 124] Train Loss: 1.8451345088859323
Thu 20 Nov 2025 12:06:15 INFO  [Epoch 125] Train Loss: 1.8427614986781449
Thu 20 Nov 2025 12:08:55 INFO  [Epoch 126] Train Loss: 1.840262223624815
Thu 20 Nov 2025 12:11:29 INFO  [Epoch 127] Train Loss: 1.8382110738961392
Thu 20 Nov 2025 12:14:04 INFO  [Epoch 128] Train Loss: 1.8361083746646822
Thu 20 Nov 2025 12:16:44 INFO  [Epoch 129] Train Loss: 1.8334869033581502
Thu 20 Nov 2025 12:19:17 INFO  [Epoch 130] Train Loss: 1.831305746176068
Thu 20 Nov 2025 12:19:17 INFO  [Epoch 130] Saved model checkpoint to ckpt/Video_Games/Nov-20-2025_01-34-754bad/Nov-20-2025_01-34-754bad_130.pth
Thu 20 Nov 2025 12:21:45 INFO  [Epoch 131] Train Loss: 1.8279771052847498
Thu 20 Nov 2025 12:24:13 INFO  [Epoch 132] Train Loss: 1.8256380814037727
Thu 20 Nov 2025 12:26:40 INFO  [Epoch 133] Train Loss: 1.823209735935259
Thu 20 Nov 2025 12:29:08 INFO  [Epoch 134] Train Loss: 1.8206068963157624
Thu 20 Nov 2025 12:31:36 INFO  [Epoch 135] Train Loss: 1.8184006154882402
Thu 20 Nov 2025 12:34:02 INFO  [Epoch 136] Train Loss: 1.815615613890891
Thu 20 Nov 2025 12:36:30 INFO  [Epoch 137] Train Loss: 1.812793651948104
Thu 20 Nov 2025 12:38:57 INFO  [Epoch 138] Train Loss: 1.8098068198181947
Thu 20 Nov 2025 12:41:23 INFO  [Epoch 139] Train Loss: 1.8075442637485888
Thu 20 Nov 2025 12:43:50 INFO  [Epoch 140] Train Loss: 1.8048098101694152
Thu 20 Nov 2025 12:43:50 INFO  [Epoch 140] Saved model checkpoint to ckpt/Video_Games/Nov-20-2025_01-34-754bad/Nov-20-2025_01-34-754bad_140.pth
Thu 20 Nov 2025 12:43:50 INFO  Best epoch: 0, Best val score: -1
Thu 20 Nov 2025 12:43:51 INFO  [Epoch 141] Saved model checkpoint to ckpt/Video_Games/Nov-20-2025_01-34-754bad/Nov-20-2025_01-34-754bad.pth
Thu 20 Nov 2025 12:43:51 INFO  Validation scores for all tokenizers: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
Thu 20 Nov 2025 13:56:06 INFO  Influence scores: tensor([[0.3895, 0.3429, 0.2413, 0.3361, 0.2599, 0.2542, 0.2426, 0.2511, 0.2057,
         0.1855, 0.2671, 0.2529, 0.2189, 0.2315, 0.2187, 0.2104, 0.2262, 0.1872,
         0.2394, 0.2139, 0.1719, 0.2031, 0.2256, 0.2298, 0.2199],
        [0.3062, 0.4374, 0.2770, 0.2971, 0.2737, 0.2767, 0.2555, 0.2728, 0.1992,
         0.1809, 0.2751, 0.2518, 0.2311, 0.2379, 0.2487, 0.2225, 0.2416, 0.1835,
         0.2453, 0.2324, 0.1924, 0.2055, 0.2455, 0.2391, 0.2344],
        [0.2891, 0.3591, 0.2931, 0.2785, 0.2694, 0.2570, 0.2518, 0.2614, 0.2084,
         0.1892, 0.2763, 0.2509, 0.2315, 0.2415, 0.2379, 0.2266, 0.2421, 0.1892,
         0.2467, 0.2306, 0.1878, 0.2058, 0.2327, 0.2304, 0.2213],
        [0.3232, 0.3208, 0.2198, 0.3578, 0.2662, 0.2518, 0.2518, 0.2618, 0.2090,
         0.1768, 0.2715, 0.2585, 0.2265, 0.2355, 0.2294, 0.2169, 0.2338, 0.1925,
         0.2506, 0.2168, 0.1737, 0.2010, 0.2275, 0.2298, 0.2202],
        [0.2589, 0.3093, 0.2235, 0.2769, 0.2945, 0.2602, 0.2589, 0.2688, 0.2152,
         0.1854, 0.2769, 0.2640, 0.2354, 0.2490, 0.2438, 0.2256, 0.2431, 0.1908,
         0.2499, 0.2254, 0.1802, 0.2034, 0.2265, 0.2224, 0.2141],
        [0.2574, 0.3176, 0.2154, 0.2663, 0.2639, 0.2931, 0.2549, 0.2636, 0.2076,
         0.1897, 0.2775, 0.2582, 0.2328, 0.2435, 0.2381, 0.2206, 0.2395, 0.1827,
         0.2376, 0.2185, 0.1765, 0.1970, 0.2286, 0.2238, 0.2155],
        [0.2513, 0.2998, 0.2147, 0.2707, 0.2643, 0.2584, 0.2942, 0.2618, 0.2204,
         0.1943, 0.2774, 0.2720, 0.2476, 0.2511, 0.2455, 0.2299, 0.2482, 0.1961,
         0.2554, 0.2238, 0.1818, 0.2091, 0.2247, 0.2314, 0.2182],
        [0.2332, 0.2924, 0.1997, 0.2556, 0.2526, 0.2427, 0.2391, 0.3325, 0.2152,
         0.1852, 0.2912, 0.2489, 0.2460, 0.2581, 0.2625, 0.2310, 0.2564, 0.2054,
         0.2539, 0.2434, 0.1972, 0.2124, 0.2557, 0.2463, 0.2336],
        [0.2253, 0.2530, 0.1811, 0.2394, 0.2332, 0.2223, 0.2325, 0.2501, 0.2572,
         0.2022, 0.2724, 0.2595, 0.2390, 0.2597, 0.2387, 0.2259, 0.2470, 0.2038,
         0.2460, 0.2230, 0.1805, 0.2068, 0.2241, 0.2195, 0.2075],
        [0.2262, 0.2568, 0.1835, 0.2293, 0.2262, 0.2260, 0.2292, 0.2439, 0.2240,
         0.2435, 0.2809, 0.2621, 0.2476, 0.2660, 0.2390, 0.2288, 0.2538, 0.2043,
         0.2478, 0.2246, 0.1854, 0.2121, 0.2309, 0.2282, 0.2146],
        [0.2113, 0.2548, 0.1744, 0.2267, 0.2206, 0.2177, 0.2148, 0.2520, 0.1981,
         0.1861, 0.3197, 0.2645, 0.2428, 0.2563, 0.2357, 0.2400, 0.2537, 0.2018,
         0.2591, 0.2375, 0.1870, 0.2085, 0.2444, 0.2409, 0.2262],
        [0.2147, 0.2489, 0.1668, 0.2309, 0.2249, 0.2157, 0.2280, 0.2276, 0.2034,
         0.1833, 0.2813, 0.3301, 0.2441, 0.2681, 0.2484, 0.2500, 0.2615, 0.2134,
         0.2716, 0.2405, 0.1947, 0.2327, 0.2400, 0.2439, 0.2357],
        [0.1977, 0.2450, 0.1640, 0.2167, 0.2116, 0.2063, 0.2184, 0.2398, 0.1982,
         0.1845, 0.2760, 0.2588, 0.2869, 0.2672, 0.2616, 0.2481, 0.2774, 0.2189,
         0.2686, 0.2472, 0.2068, 0.2288, 0.2472, 0.2490, 0.2332],
        [0.1890, 0.2289, 0.1520, 0.2043, 0.2036, 0.1957, 0.1993, 0.2303, 0.1979,
         0.1828, 0.2679, 0.2614, 0.2462, 0.2973, 0.2645, 0.2439, 0.2732, 0.2223,
         0.2604, 0.2468, 0.2067, 0.2308, 0.2486, 0.2418, 0.2318],
        [0.1870, 0.2510, 0.1592, 0.2099, 0.2124, 0.2026, 0.2081, 0.2470, 0.1897,
         0.1667, 0.2591, 0.2533, 0.2525, 0.2768, 0.3306, 0.2517, 0.2939, 0.2340,
         0.2738, 0.2699, 0.2391, 0.2508, 0.2774, 0.2662, 0.2611],
        [0.1795, 0.2262, 0.1495, 0.1973, 0.1946, 0.1864, 0.1935, 0.2174, 0.1784,
         0.1586, 0.2649, 0.2563, 0.2402, 0.2568, 0.2531, 0.2852, 0.2843, 0.2313,
         0.2780, 0.2649, 0.2160, 0.2400, 0.2595, 0.2580, 0.2412],
        [0.1758, 0.2256, 0.1449, 0.1938, 0.1905, 0.1830, 0.1891, 0.2198, 0.1765,
         0.1621, 0.2569, 0.2459, 0.2472, 0.2654, 0.2738, 0.2630, 0.3171, 0.2454,
         0.2828, 0.2758, 0.2404, 0.2588, 0.2796, 0.2746, 0.2601],
        [0.1794, 0.2108, 0.1361, 0.1969, 0.1851, 0.1728, 0.1840, 0.2157, 0.1792,
         0.1578, 0.2504, 0.2439, 0.2348, 0.2594, 0.2598, 0.2553, 0.2900, 0.2803,
         0.2924, 0.2760, 0.2323, 0.2678, 0.2740, 0.2784, 0.2603],
        [0.1840, 0.2233, 0.1446, 0.2075, 0.1948, 0.1790, 0.1949, 0.2161, 0.1733,
         0.1518, 0.2605, 0.2552, 0.2363, 0.2494, 0.2519, 0.2552, 0.2812, 0.2462,
         0.3570, 0.2864, 0.2385, 0.2713, 0.2877, 0.2960, 0.2790],
        [0.1767, 0.2299, 0.1473, 0.1918, 0.1875, 0.1773, 0.1792, 0.2226, 0.1676,
         0.1478, 0.2561, 0.2407, 0.2319, 0.2538, 0.2647, 0.2581, 0.2890, 0.2454,
         0.3032, 0.3325, 0.2734, 0.2841, 0.3170, 0.3046, 0.2908],
        [0.1748, 0.2311, 0.1453, 0.1902, 0.1851, 0.1766, 0.1810, 0.2183, 0.1674,
         0.1507, 0.2483, 0.2370, 0.2344, 0.2560, 0.2756, 0.2515, 0.2954, 0.2438,
         0.2981, 0.3149, 0.3188, 0.3082, 0.3472, 0.3302, 0.3238],
        [0.1713, 0.2096, 0.1290, 0.1829, 0.1722, 0.1626, 0.1712, 0.1989, 0.1581,
         0.1425, 0.2349, 0.2394, 0.2206, 0.2436, 0.2531, 0.2414, 0.2798, 0.2451,
         0.2962, 0.2915, 0.2733, 0.3474, 0.3291, 0.3379, 0.3304],
        [0.1766, 0.2319, 0.1369, 0.1902, 0.1764, 0.1735, 0.1672, 0.2223, 0.1557,
         0.1420, 0.2501, 0.2276, 0.2196, 0.2422, 0.2604, 0.2409, 0.2806, 0.2317,
         0.2935, 0.3049, 0.2948, 0.3089, 0.3829, 0.3507, 0.3470],
        [0.1819, 0.2268, 0.1358, 0.1944, 0.1729, 0.1716, 0.1750, 0.2147, 0.1525,
         0.1406, 0.2487, 0.2316, 0.2227, 0.2378, 0.2507, 0.2412, 0.2777, 0.2378,
         0.3027, 0.2957, 0.2786, 0.3214, 0.3536, 0.3955, 0.3695],
        [0.1815, 0.2318, 0.1370, 0.1949, 0.1758, 0.1719, 0.1725, 0.2124, 0.1508,
         0.1368, 0.2444, 0.2346, 0.2175, 0.2373, 0.2551, 0.2352, 0.2728, 0.2296,
         0.2953, 0.2908, 0.2811, 0.3225, 0.3585, 0.3782, 0.4016]],
       device='cuda:0')
Thu 20 Nov 2025 13:56:06 INFO  Mean influence score: [0.23155806958675385, 0.2301257997751236, 0.2320837676525116, 0.22718726098537445, 0.22495192289352417, 0.22317470610141754, 0.22004340589046478, 0.22230292856693268, 0.22191819548606873, 0.22823558747768402, 0.22318412363529205, 0.22104422748088837, 0.22382752597332, 0.2253989577293396, 0.22323977947235107, 0.22797462344169617, 0.22633308172225952, 0.22369319200515747, 0.2234262377023697, 0.22983196377754211, 0.22754186391830444, 0.2220260500907898, 0.22365310788154602, 0.22407419979572296, 0.21722714602947235]
Thu 20 Nov 2025 13:56:06 INFO  Stage 4 selected prob: [0.04008801281452179, 0.04006887972354889, 0.04009503871202469, 0.04002964869141579, 0.039999835193157196, 0.03997614234685898, 0.039934441447257996, 0.03996453434228897, 0.03995940834283829, 0.04004364088177681, 0.039976269006729126, 0.039947763085365295, 0.039984848350286484, 0.040005795657634735, 0.03997701406478882, 0.04004015401005745, 0.04001825675368309, 0.039983056485652924, 0.03997949883341789, 0.04006495699286461, 0.04003438726067543, 0.039960842579603195, 0.039982520043849945, 0.03998813405632973, 0.039896972477436066]
Thu 20 Nov 2025 13:58:34 INFO  [Epoch 141] Train Loss: 1.801928276490981
Thu 20 Nov 2025 14:00:57 INFO  [Epoch 142] Train Loss: 1.8000581801858189
Thu 20 Nov 2025 14:03:24 INFO  [Epoch 143] Train Loss: 1.7972212404472947
Thu 20 Nov 2025 14:05:51 INFO  [Epoch 144] Train Loss: 1.7940548832121963
Thu 20 Nov 2025 14:08:16 INFO  [Epoch 145] Train Loss: 1.7912717431093275
Thu 20 Nov 2025 14:10:42 INFO  [Epoch 146] Train Loss: 1.788975575485745
Thu 20 Nov 2025 14:13:09 INFO  [Epoch 147] Train Loss: 1.7857689478337535
Thu 20 Nov 2025 14:15:34 INFO  [Epoch 148] Train Loss: 1.7834487528644474
Thu 20 Nov 2025 14:17:58 INFO  [Epoch 149] Train Loss: 1.7803670272058503
Thu 20 Nov 2025 14:20:26 INFO  [Epoch 150] Train Loss: 1.7777540388599786
Thu 20 Nov 2025 14:20:26 INFO  [Epoch 150] Saved model checkpoint to ckpt/Video_Games/Nov-20-2025_01-34-754bad/Nov-20-2025_01-34-754bad_150.pth
Thu 20 Nov 2025 14:22:52 INFO  [Epoch 151] Train Loss: 1.7737487587344232
Thu 20 Nov 2025 14:25:17 INFO  [Epoch 152] Train Loss: 1.7706531782753219
Thu 20 Nov 2025 14:27:43 INFO  [Epoch 153] Train Loss: 1.7686918518833212
Thu 20 Nov 2025 14:30:09 INFO  [Epoch 154] Train Loss: 1.7649495441135752
Thu 20 Nov 2025 14:32:35 INFO  [Epoch 155] Train Loss: 1.7628051045433435
Thu 20 Nov 2025 14:35:03 INFO  [Epoch 156] Train Loss: 1.7582512943210749
Thu 20 Nov 2025 14:37:28 INFO  [Epoch 157] Train Loss: 1.7562360871712674
Thu 20 Nov 2025 14:39:54 INFO  [Epoch 158] Train Loss: 1.7530314113412584
Thu 20 Nov 2025 14:42:21 INFO  [Epoch 159] Train Loss: 1.749925889218636
Thu 20 Nov 2025 14:44:47 INFO  [Epoch 160] Train Loss: 1.7464403730446767
Thu 20 Nov 2025 14:44:47 INFO  [Epoch 160] Saved model checkpoint to ckpt/Video_Games/Nov-20-2025_01-34-754bad/Nov-20-2025_01-34-754bad_160.pth
Thu 20 Nov 2025 14:44:47 INFO  Best epoch: 0, Best val score: -1
Thu 20 Nov 2025 14:44:47 INFO  [Epoch 161] Saved model checkpoint to ckpt/Video_Games/Nov-20-2025_01-34-754bad/Nov-20-2025_01-34-754bad.pth
Thu 20 Nov 2025 14:44:47 INFO  Validation scores for all tokenizers: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
Thu 20 Nov 2025 15:56:20 INFO  Influence scores: tensor([[ 0.0757,  0.0149,  0.0295,  0.0093, -0.0465, -0.0259, -0.0105, -0.0922,
         -0.0427, -0.0768, -0.1213, -0.0684, -0.0894, -0.1130, -0.1100, -0.0742,
         -0.0974, -0.0608, -0.0225, -0.1451, -0.0920, -0.1034, -0.0570, -0.0646,
         -0.1057],
        [-0.0496,  0.1240,  0.0543, -0.0569, -0.0499, -0.0163, -0.0198, -0.0828,
         -0.0764, -0.1081, -0.1347, -0.0949, -0.0973, -0.1304, -0.0950, -0.0811,
         -0.0991, -0.0894, -0.0415, -0.1433, -0.0884, -0.1254, -0.0550, -0.0743,
         -0.1115],
        [-0.0610,  0.0273,  0.0850, -0.0701, -0.0463, -0.0345, -0.0156, -0.0878,
         -0.0559, -0.0871, -0.1230, -0.0867, -0.0895, -0.1164, -0.1019, -0.0674,
         -0.0904, -0.0726, -0.0301, -0.1375, -0.0850, -0.1165, -0.0617, -0.0779,
         -0.1179],
        [-0.0099, -0.0117,  0.0013,  0.0413, -0.0372, -0.0298,  0.0008, -0.0804,
         -0.0397, -0.0893, -0.1173, -0.0624, -0.0803, -0.1094, -0.0969, -0.0687,
         -0.0904, -0.0541, -0.0096, -0.1417, -0.0901, -0.1054, -0.0562, -0.0639,
         -0.1059],
        [-0.0933, -0.0342, -0.0053, -0.0662, -0.0106, -0.0283, -0.0063, -0.0787,
         -0.0438, -0.0889, -0.1210, -0.0671, -0.0820, -0.1049, -0.0879, -0.0673,
         -0.0889, -0.0655, -0.0218, -0.1405, -0.0900, -0.1144, -0.0655, -0.0849,
         -0.1231],
        [-0.0961, -0.0235, -0.0148, -0.0819, -0.0509,  0.0161, -0.0084, -0.0854,
         -0.0518, -0.0824, -0.1196, -0.0732, -0.0826, -0.1088, -0.0957, -0.0725,
         -0.0925, -0.0766, -0.0379, -0.1491, -0.0949, -0.1203, -0.0637, -0.0810,
         -0.1210],
        [-0.1062, -0.0508, -0.0215, -0.0764, -0.0544, -0.0335,  0.0376, -0.0912,
         -0.0381, -0.0797, -0.1227, -0.0576, -0.0671, -0.1041, -0.0892, -0.0634,
         -0.0847, -0.0611, -0.0173, -0.1458, -0.0898, -0.1089, -0.0704, -0.0751,
         -0.1207],
        [-0.1392, -0.0646, -0.0443, -0.1089, -0.0772, -0.0627, -0.0425, -0.0018,
         -0.0526, -0.0967, -0.1098, -0.0987, -0.0765, -0.1023, -0.0752, -0.0673,
         -0.0800, -0.0560, -0.0268, -0.1270, -0.0781, -0.1124, -0.0386, -0.0622,
         -0.1087],
        [-0.1365, -0.1092, -0.0601, -0.1164, -0.0909, -0.0773, -0.0374, -0.1000,
          0.0172, -0.0607, -0.1240, -0.0699, -0.0710, -0.0850, -0.0923, -0.0620,
         -0.0788, -0.0440, -0.0241, -0.1408, -0.0867, -0.1049, -0.0656, -0.0849,
         -0.1297],
        [-0.1439, -0.1105, -0.0646, -0.1366, -0.1062, -0.0785, -0.0479, -0.1136,
         -0.0329, -0.0158, -0.1191, -0.0743, -0.0671, -0.0837, -0.0996, -0.0644,
         -0.0756, -0.0503, -0.0303, -0.1452, -0.0865, -0.1056, -0.0626, -0.0811,
         -0.1272],
        [-0.1511, -0.1010, -0.0639, -0.1280, -0.1020, -0.0790, -0.0552, -0.0922,
         -0.0583, -0.0815, -0.0569, -0.0603, -0.0632, -0.0857, -0.0919, -0.0404,
         -0.0658, -0.0442, -0.0012, -0.1163, -0.0734, -0.0988, -0.0349, -0.0505,
         -0.0989],
        [-0.1413, -0.1046, -0.0698, -0.1158, -0.0925, -0.0754, -0.0339, -0.1237,
         -0.0453, -0.0813, -0.1052,  0.0330, -0.0577, -0.0675, -0.0720, -0.0256,
         -0.0546, -0.0251,  0.0199, -0.1098, -0.0610, -0.0616, -0.0366, -0.0433,
         -0.0821],
        [-0.1773, -0.1226, -0.0880, -0.1499, -0.1222, -0.1021, -0.0606, -0.1182,
         -0.0658, -0.0916, -0.1238, -0.0751, -0.0146, -0.0803, -0.0672, -0.0375,
         -0.0438, -0.0293,  0.0015, -0.1135, -0.0547, -0.0814, -0.0377, -0.0499,
         -0.0990],
        [-0.1830, -0.1398, -0.0984, -0.1613, -0.1291, -0.1108, -0.0796, -0.1263,
         -0.0608, -0.0881, -0.1295, -0.0660, -0.0620, -0.0370, -0.0605, -0.0394,
         -0.0446, -0.0201, -0.0050, -0.1098, -0.0532, -0.0739, -0.0334, -0.0553,
         -0.0980],
        [-0.1914, -0.1152, -0.0953, -0.1600, -0.1231, -0.1078, -0.0746, -0.1110,
         -0.0784, -0.1158, -0.1461, -0.0824, -0.0589, -0.0706,  0.0197, -0.0354,
         -0.0247, -0.0124,  0.0061, -0.0862, -0.0154, -0.0555, -0.0012, -0.0283,
         -0.0656],
        [-0.1862, -0.1321, -0.0913, -0.1613, -0.1318, -0.1150, -0.0795, -0.1337,
         -0.0793, -0.1109, -0.1251, -0.0657, -0.0616, -0.0812, -0.0662,  0.0221,
         -0.0227, -0.0012,  0.0279, -0.0774, -0.0316, -0.0526, -0.0090, -0.0242,
         -0.0738],
        [-0.2033, -0.1458, -0.1093, -0.1774, -0.1473, -0.1301, -0.0960, -0.1399,
         -0.0911, -0.1171, -0.1457, -0.0892, -0.0626, -0.0807, -0.0494, -0.0169,
          0.0097,  0.0061,  0.0210, -0.0752, -0.0104, -0.0410,  0.0057, -0.0144,
         -0.0620],
        [-0.1926, -0.1593, -0.1145, -0.1666, -0.1490, -0.1381, -0.0957, -0.1419,
         -0.0811, -0.1156, -0.1478, -0.0849, -0.0723, -0.0821, -0.0629, -0.0208,
         -0.0195,  0.0561,  0.0407, -0.0711, -0.0173, -0.0244,  0.0035, -0.0040,
         -0.0566],
        [-0.1860, -0.1423, -0.1041, -0.1515, -0.1354, -0.1302, -0.0834, -0.1421,
         -0.0914, -0.1269, -0.1358, -0.0722, -0.0730, -0.0966, -0.0724, -0.0220,
         -0.0327,  0.0113,  0.1251, -0.0567, -0.0096, -0.0206,  0.0205,  0.0183,
         -0.0319],
        [-0.2038, -0.1404, -0.1073, -0.1809, -0.1510, -0.1386, -0.1090, -0.1387,
         -0.1048, -0.1373, -0.1468, -0.0972, -0.0847, -0.0977, -0.0621, -0.0230,
         -0.0261,  0.0031,  0.0469, -0.0046,  0.0281, -0.0108,  0.0519,  0.0224,
         -0.0239],
        [-0.2069, -0.1398, -0.1108, -0.1841, -0.1549, -0.1395, -0.1074, -0.1458,
         -0.1065, -0.1346, -0.1595, -0.1014, -0.0813, -0.0963, -0.0488, -0.0325,
         -0.0183,  0.0011,  0.0391, -0.0278,  0.0867,  0.0182,  0.0888,  0.0545,
          0.0163],
        [-0.2001, -0.1576, -0.1213, -0.1802, -0.1612, -0.1467, -0.1076, -0.1607,
         -0.1047, -0.1351, -0.1654, -0.0859, -0.0877, -0.0999, -0.0686, -0.0356,
         -0.0283,  0.0144,  0.0484, -0.0474,  0.0387,  0.0810,  0.0784,  0.0758,
          0.0377],
        [-0.2098, -0.1429, -0.1241, -0.1887, -0.1697, -0.1472, -0.1267, -0.1438,
         -0.1235, -0.1483, -0.1584, -0.1173, -0.1030, -0.1164, -0.0721, -0.0499,
         -0.0403, -0.0173,  0.0315, -0.0429,  0.0506,  0.0197,  0.1317,  0.0791,
          0.0444],
        [-0.1948, -0.1407, -0.1187, -0.1743, -0.1669, -0.1433, -0.1115, -0.1473,
         -0.1214, -0.1459, -0.1544, -0.1035, -0.0942, -0.1168, -0.0781, -0.0426,
         -0.0380, -0.0020,  0.0509, -0.0493,  0.0391,  0.0400,  0.1022,  0.1430,
          0.0795],
        [-0.1879, -0.1299, -0.1101, -0.1672, -0.1578, -0.1340, -0.1069, -0.1452,
         -0.1168, -0.1439, -0.1538, -0.0929, -0.0941, -0.1120, -0.0664, -0.0451,
         -0.0378, -0.0060,  0.0493, -0.0488,  0.0483,  0.0500,  0.1142,  0.1275,
          0.1300]], device='cuda:0')
Thu 20 Nov 2025 15:56:20 INFO  Mean influence score: [0.22071245312690735, 0.21895653009414673, 0.22087468206882477, 0.21647964417934418, 0.21391944587230682, 0.21207717061042786, 0.20917637646198273, 0.21117614209651947, 0.21062003076076508, 0.21657031774520874, 0.21204733848571777, 0.21037787199020386, 0.21250292658805847, 0.21392612159252167, 0.2121979147195816, 0.2167060375213623, 0.21497821807861328, 0.21250353753566742, 0.21250849962234497, 0.2185346931219101, 0.21654552221298218, 0.2111932635307312, 0.21251323819160461, 0.21321094036102295, 0.20684486627578735]
Thu 20 Nov 2025 15:56:20 INFO  Stage 5 selected prob: [0.040091097354888916, 0.0400676354765892, 0.04009326547384262, 0.040034569799900055, 0.040000420063734055, 0.039975859224796295, 0.039937227964401245, 0.03996386006474495, 0.03995644673705101, 0.04003577679395676, 0.03997546434402466, 0.03995322436094284, 0.03998153656721115, 0.04000050947070122, 0.03997746855020523, 0.04003759101033211, 0.04001453518867493, 0.03998154401779175, 0.03998161107301712, 0.04006200283765793, 0.04003544896841049, 0.039964087307453156, 0.03998167812824249, 0.039990976452827454, 0.03990619629621506]
Thu 20 Nov 2025 15:58:45 INFO  [Epoch 161] Train Loss: 1.7435000212611378
Thu 20 Nov 2025 16:01:12 INFO  [Epoch 162] Train Loss: 1.739784271618114
Thu 20 Nov 2025 16:03:38 INFO  [Epoch 163] Train Loss: 1.736739477538234
Thu 20 Nov 2025 16:06:02 INFO  [Epoch 164] Train Loss: 1.7338889903428472
Thu 20 Nov 2025 16:08:27 INFO  [Epoch 165] Train Loss: 1.7306227934291465
Thu 20 Nov 2025 16:10:55 INFO  [Epoch 166] Train Loss: 1.7268251090896636
Thu 20 Nov 2025 16:13:14 INFO  [Epoch 167] Train Loss: 1.7241516401523789
Thu 20 Nov 2025 16:15:42 INFO  [Epoch 168] Train Loss: 1.7204590704450275
Thu 20 Nov 2025 16:18:08 INFO  [Epoch 169] Train Loss: 1.717183635904522
Thu 20 Nov 2025 16:20:30 INFO  [Epoch 170] Train Loss: 1.7132026736110333
Thu 20 Nov 2025 16:20:30 INFO  [Epoch 170] Saved model checkpoint to ckpt/Video_Games/Nov-20-2025_01-34-754bad/Nov-20-2025_01-34-754bad_170.pth
Thu 20 Nov 2025 16:22:55 INFO  [Epoch 171] Train Loss: 1.7098665774213762
Thu 20 Nov 2025 16:25:23 INFO  [Epoch 172] Train Loss: 1.706815297826837
Thu 20 Nov 2025 16:27:45 INFO  [Epoch 173] Train Loss: 1.703383739332895
Thu 20 Nov 2025 16:30:12 INFO  [Epoch 174] Train Loss: 1.700478877167444
Thu 20 Nov 2025 16:32:38 INFO  [Epoch 175] Train Loss: 1.6970936557616045
Thu 20 Nov 2025 16:35:03 INFO  [Epoch 176] Train Loss: 1.6939371964070788
Thu 20 Nov 2025 16:37:30 INFO  [Epoch 177] Train Loss: 1.6905851987222906
Thu 20 Nov 2025 16:39:57 INFO  [Epoch 178] Train Loss: 1.6877795889916107
Thu 20 Nov 2025 16:42:21 INFO  [Epoch 179] Train Loss: 1.6850618550906311
Thu 20 Nov 2025 16:44:47 INFO  [Epoch 180] Train Loss: 1.6817850944043127
Thu 20 Nov 2025 16:44:48 INFO  [Epoch 180] Saved model checkpoint to ckpt/Video_Games/Nov-20-2025_01-34-754bad/Nov-20-2025_01-34-754bad_180.pth
Thu 20 Nov 2025 16:44:48 INFO  Best epoch: 0, Best val score: -1
Thu 20 Nov 2025 16:44:48 INFO  [Epoch 181] Saved model checkpoint to ckpt/Video_Games/Nov-20-2025_01-34-754bad/Nov-20-2025_01-34-754bad.pth
Thu 20 Nov 2025 16:44:48 INFO  Validation scores for all tokenizers: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
Thu 20 Nov 2025 17:57:13 INFO  Influence scores: tensor([[ 0.2787,  0.1370,  0.1379,  0.1863,  0.0986,  0.0550,  0.1693,  0.0940,
          0.1209,  0.1097,  0.1686,  0.2095,  0.1181,  0.0803,  0.0586,  0.1026,
          0.0444,  0.0607,  0.0745,  0.0374,  0.1362,  0.1029,  0.0525,  0.0845,
          0.0483],
        [ 0.1086,  0.2603,  0.1567,  0.0922,  0.0819,  0.0570,  0.1517,  0.0973,
          0.0698,  0.0627,  0.1426,  0.1696,  0.1012,  0.0514,  0.0706,  0.0863,
          0.0347,  0.0128,  0.0433,  0.0302,  0.1324,  0.0685,  0.0474,  0.0649,
          0.0357],
        [ 0.0974,  0.1430,  0.1960,  0.0763,  0.0853,  0.0364,  0.1551,  0.0891,
          0.0947,  0.0876,  0.1568,  0.1790,  0.1103,  0.0678,  0.0608,  0.1022,
          0.0438,  0.0332,  0.0563,  0.0381,  0.1357,  0.0799,  0.0369,  0.0595,
          0.0243],
        [ 0.1511,  0.0848,  0.0832,  0.2079,  0.0923,  0.0334,  0.1690,  0.0953,
          0.1093,  0.0755,  0.1571,  0.2011,  0.1154,  0.0687,  0.0584,  0.0934,
          0.0370,  0.0500,  0.0757,  0.0233,  0.1211,  0.0851,  0.0353,  0.0679,
          0.0311],
        [ 0.0405,  0.0514,  0.0705,  0.0681,  0.1210,  0.0321,  0.1550,  0.0917,
          0.1012,  0.0733,  0.1495,  0.1933,  0.1089,  0.0717,  0.0650,  0.0919,
          0.0365,  0.0308,  0.0568,  0.0214,  0.1174,  0.0709,  0.0198,  0.0390,
          0.0046],
        [ 0.0400,  0.0678,  0.0616,  0.0496,  0.0723,  0.0900,  0.1540,  0.0845,
          0.0905,  0.0829,  0.1526,  0.1862,  0.1089,  0.0670,  0.0577,  0.0872,
          0.0330,  0.0165,  0.0369,  0.0126,  0.1136,  0.0651,  0.0252,  0.0453,
          0.0092],
        [ 0.0266,  0.0325,  0.0519,  0.0560,  0.0663,  0.0258,  0.2119,  0.0767,
          0.1058,  0.0846,  0.1470,  0.2034,  0.1275,  0.0705,  0.0648,  0.0970,
          0.0411,  0.0356,  0.0625,  0.0141,  0.1171,  0.0779,  0.0144,  0.0504,
          0.0074],
        [-0.0155,  0.0142,  0.0222,  0.0175,  0.0379, -0.0090,  0.1127,  0.1941,
          0.0903,  0.0646,  0.1632,  0.1539,  0.1167,  0.0742,  0.0814,  0.0920,
          0.0456,  0.0409,  0.0513,  0.0374,  0.1302,  0.0724,  0.0535,  0.0678,
          0.0241],
        [ 0.0015, -0.0271,  0.0153,  0.0191,  0.0343, -0.0164,  0.1279,  0.0774,
          0.1906,  0.1212,  0.1587,  0.2024,  0.1334,  0.1073,  0.0730,  0.1099,
          0.0604,  0.0689,  0.0668,  0.0329,  0.1331,  0.0934,  0.0308,  0.0496,
          0.0082],
        [-0.0095, -0.0328,  0.0076, -0.0095,  0.0110, -0.0206,  0.1120,  0.0555,
          0.1242,  0.1756,  0.1628,  0.1937,  0.1341,  0.1063,  0.0602,  0.1043,
          0.0613,  0.0582,  0.0546,  0.0248,  0.1297,  0.0898,  0.0311,  0.0529,
          0.0081],
        [-0.0373, -0.0354, -0.0066, -0.0161,  0.0013, -0.0354,  0.0885,  0.0700,
          0.0769,  0.0784,  0.2233,  0.1956,  0.1259,  0.0883,  0.0517,  0.1183,
          0.0563,  0.0488,  0.0751,  0.0430,  0.1300,  0.0826,  0.0502,  0.0733,
          0.0271],
        [-0.0309, -0.0458, -0.0189, -0.0069,  0.0086, -0.0383,  0.1099,  0.0250,
          0.0850,  0.0714,  0.1588,  0.3051,  0.1287,  0.1057,  0.0736,  0.1307,
          0.0693,  0.0692,  0.0945,  0.0477,  0.1444,  0.1230,  0.0443,  0.0780,
          0.0434],
        [-0.0677, -0.0613, -0.0345, -0.0401, -0.0226, -0.0630,  0.0838,  0.0378,
          0.0687,  0.0659,  0.1416,  0.1795,  0.1896,  0.0967,  0.0880,  0.1270,
          0.0882,  0.0704,  0.0826,  0.0512,  0.1552,  0.1089,  0.0500,  0.0790,
          0.0305],
        [-0.0706, -0.0751, -0.0423, -0.0511, -0.0253, -0.0692,  0.0637,  0.0327,
          0.0804,  0.0759,  0.1405,  0.1947,  0.1325,  0.1556,  0.1000,  0.1264,
          0.0917,  0.0858,  0.0768,  0.0600,  0.1630,  0.1201,  0.0586,  0.0750,
          0.0356],
        [-0.0880, -0.0541, -0.0447, -0.0549, -0.0243, -0.0732,  0.0637,  0.0445,
          0.0509,  0.0339,  0.1108,  0.1683,  0.1295,  0.1071,  0.1939,  0.1241,
          0.1086,  0.0873,  0.0825,  0.0794,  0.2007,  0.1363,  0.0919,  0.1012,
          0.0689],
        [-0.0877, -0.0813, -0.0471, -0.0649, -0.0418, -0.0879,  0.0517,  0.0097,
          0.0434,  0.0334,  0.1320,  0.1819,  0.1213,  0.0869,  0.0794,  0.1898,
          0.1060,  0.0958,  0.1047,  0.0864,  0.1777,  0.1347,  0.0750,  0.1016,
          0.0520],
        [-0.0987, -0.0867, -0.0597, -0.0749, -0.0525, -0.0968,  0.0405,  0.0120,
          0.0400,  0.0370,  0.1162,  0.1631,  0.1296,  0.0986,  0.1098,  0.1514,
          0.1559,  0.1154,  0.1066,  0.0996,  0.2118,  0.1586,  0.1037,  0.1220,
          0.0764],
        [-0.0733, -0.0956, -0.0570, -0.0496, -0.0443, -0.0991,  0.0497,  0.0182,
          0.0599,  0.0455,  0.1216,  0.1767,  0.1238,  0.1030,  0.1007,  0.1530,
          0.1267,  0.1877,  0.1409,  0.1134,  0.2113,  0.1888,  0.1084,  0.1443,
          0.0932],
        [-0.0752, -0.0830, -0.0539, -0.0409, -0.0389, -0.0985,  0.0567,  0.0102,
          0.0376,  0.0207,  0.1274,  0.1839,  0.1162,  0.0754,  0.0786,  0.1428,
          0.0989,  0.1236,  0.2348,  0.1223,  0.2121,  0.1831,  0.1196,  0.1626,
          0.1133],
        [-0.0948, -0.0769, -0.0531, -0.0748, -0.0538, -0.1036,  0.0275,  0.0165,
          0.0243,  0.0140,  0.1149,  0.1560,  0.1051,  0.0791,  0.0958,  0.1447,
          0.1115,  0.1146,  0.1414,  0.1903,  0.2622,  0.1994,  0.1630,  0.1719,
          0.1276],
        [-0.1085, -0.0854, -0.0665, -0.0868, -0.0674, -0.1152,  0.0207, -0.0021,
          0.0131,  0.0078,  0.0910,  0.1424,  0.0985,  0.0709,  0.1041,  0.1249,
          0.1126,  0.1018,  0.1218,  0.1504,  0.3239,  0.2261,  0.2007,  0.2010,
          0.1677],
        [-0.0819, -0.0892, -0.0616, -0.0653, -0.0562, -0.1047,  0.0388, -0.0012,
          0.0327,  0.0256,  0.1028,  0.1779,  0.1101,  0.0859,  0.0995,  0.1401,
          0.1198,  0.1399,  0.1524,  0.1491,  0.2861,  0.3241,  0.2063,  0.2467,
          0.2136],
        [-0.0978, -0.0758, -0.0707, -0.0796, -0.0724, -0.1090,  0.0109,  0.0189,
          0.0061,  0.0053,  0.1084,  0.1354,  0.0870,  0.0614,  0.0881,  0.1182,
          0.0981,  0.0940,  0.1261,  0.1470,  0.2949,  0.2409,  0.2691,  0.2468,
          0.2178],
        [-0.0758, -0.0693, -0.0596, -0.0575, -0.0650, -0.1016,  0.0339,  0.0165,
          0.0109,  0.0118,  0.1161,  0.1552,  0.1019,  0.0641,  0.0868,  0.1296,
          0.1041,  0.1170,  0.1536,  0.1438,  0.2839,  0.2693,  0.2359,  0.3309,
          0.2663],
        [-0.0719, -0.0566, -0.0530, -0.0521, -0.0563, -0.0936,  0.0363,  0.0169,
          0.0136,  0.0099,  0.1141,  0.1658,  0.0993,  0.0678,  0.0988,  0.1250,
          0.1024,  0.1104,  0.1477,  0.1423,  0.2939,  0.2798,  0.2493,  0.3100,
          0.3279]], device='cuda:0')
Thu 20 Nov 2025 17:57:13 INFO  Mean influence score: [0.21967053413391113, 0.217727929353714, 0.21963384747505188, 0.21530963480472565, 0.21260683238506317, 0.21075493097305298, 0.20790372788906097, 0.20983341336250305, 0.20933516323566437, 0.21515820920467377, 0.21063582599163055, 0.2090584933757782, 0.21106116473674774, 0.212482288479805, 0.21081149578094482, 0.21520458161830902, 0.2135409116744995, 0.21119147539138794, 0.21118927001953125, 0.21714845299720764, 0.2151571810245514, 0.21002362668514252, 0.21120913326740265, 0.2120266556739807, 0.20576804876327515]
Thu 20 Nov 2025 17:57:13 INFO  Stage 6 selected prob: [0.040094658732414246, 0.04006870090961456, 0.04009417071938515, 0.04003642126917839, 0.04000036418437958, 0.03997568041086197, 0.03993770852684975, 0.03996340185403824, 0.03995676338672638, 0.04003439471125603, 0.039974089711904526, 0.0399530753493309, 0.039979759603738785, 0.03999870643019676, 0.03997642919421196, 0.04003501683473587, 0.040012821555137634, 0.03998149186372757, 0.03998146951198578, 0.04006096348166466, 0.04003438353538513, 0.039965931326150894, 0.03998173400759697, 0.03999262675642967, 0.039909280836582184]
Thu 20 Nov 2025 17:59:41 INFO  [Epoch 181] Train Loss: 1.6786185414404482
Thu 20 Nov 2025 18:02:09 INFO  [Epoch 182] Train Loss: 1.6765704750439374
Thu 20 Nov 2025 18:04:37 INFO  [Epoch 183] Train Loss: 1.6729197308248536
Thu 20 Nov 2025 18:07:04 INFO  [Epoch 184] Train Loss: 1.6707052595366843
Thu 20 Nov 2025 18:09:32 INFO  [Epoch 185] Train Loss: 1.6690999627343477
Thu 20 Nov 2025 18:11:57 INFO  [Epoch 186] Train Loss: 1.666079624624326
Thu 20 Nov 2025 18:14:25 INFO  [Epoch 187] Train Loss: 1.6641581046995508
Thu 20 Nov 2025 18:16:52 INFO  [Epoch 188] Train Loss: 1.6618885082052481
Thu 20 Nov 2025 18:19:17 INFO  [Epoch 189] Train Loss: 1.660501826281253
Thu 20 Nov 2025 18:21:45 INFO  [Epoch 190] Train Loss: 1.6582348666596136
Thu 20 Nov 2025 18:21:45 INFO  [Epoch 190] Saved model checkpoint to ckpt/Video_Games/Nov-20-2025_01-34-754bad/Nov-20-2025_01-34-754bad_190.pth
Thu 20 Nov 2025 18:24:13 INFO  [Epoch 191] Train Loss: 1.6567983706715483
Thu 20 Nov 2025 18:26:36 INFO  [Epoch 192] Train Loss: 1.6555407012176329
Thu 20 Nov 2025 18:29:04 INFO  [Epoch 193] Train Loss: 1.6540192886324003
Thu 20 Nov 2025 18:31:30 INFO  [Epoch 194] Train Loss: 1.653771561315161
Thu 20 Nov 2025 18:33:56 INFO  [Epoch 195] Train Loss: 1.6522301618426922
Thu 20 Nov 2025 18:36:24 INFO  [Epoch 196] Train Loss: 1.6520799640638026
Thu 20 Nov 2025 18:38:51 INFO  [Epoch 197] Train Loss: 1.6515894240509128
Thu 20 Nov 2025 18:41:17 INFO  [Epoch 198] Train Loss: 1.6511609390320465
Thu 20 Nov 2025 18:43:44 INFO  [Epoch 199] Train Loss: 1.6507574764572976
Thu 20 Nov 2025 18:46:11 INFO  [Epoch 200] Train Loss: 1.6508029374261621
Thu 20 Nov 2025 18:46:12 INFO  [Epoch 200] Saved model checkpoint to ckpt/Video_Games/Nov-20-2025_01-34-754bad/Nov-20-2025_01-34-754bad_200.pth
