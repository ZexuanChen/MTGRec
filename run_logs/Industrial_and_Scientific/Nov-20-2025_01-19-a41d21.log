Thu 20 Nov 2025 01:19:51 INFO  Device: cuda
Thu 20 Nov 2025 01:19:51 INFO  Config: {'rand_seed': 2024, 'reproducibility': True, 'num_proc': 1, 'data_dir': 'datasets/Industrial_and_Scientific', 'log_dir': 'run_logs/', 'tensorboard_log_dir': 'tensorboard/', 'ckpt_dir': 'ckpt/Industrial_and_Scientific/Nov-20-2025_01-19-452d96', 'stage': 'pretrain', 'pretrained_model': '', 'epoch_per_stage': [60, 20, 20, 20, 20, 20, 20], 'tau': 3.0, 'load_best_for_next_stage': False, 'train_batch_size': 256, 'eval_batch_size': 256, 'lr': 0.005, 'weight_decay': 0.05, 'warmup_steps': 10000, 'steps': None, 'epochs': 200, 'max_grad_norm': 1.0, 'eval_interval': 1, 'save_interval': 10, 'patience': 50, 'topk': [5, 10], 'metrics': ['ndcg', 'recall'], 'val_metric': 'ndcg@10', 'val_ratio': 1.0, 'val_delay': 199, 'n_codebooks': 3, 'codebook_size': 256, 'expand_final': True, 'token_prefix': 'rqvae/sentence-t5-base_256,256,256,256', 'token_suffix': 'sem_ids', 'n_user_tokens': 1, 'max_item_seq_len': 20, 'num_beams': 20, 'test_num_beams': None, 'num_layers': 5, 'num_decoder_layers': 5, 'd_model': 128, 'd_ff': 512, 'num_heads': 4, 'd_kv': 64, 'dropout_rate': 0.1, 'activation_function': 'relu', 'feed_forward_proj': 'relu', 'results_dir': None, 'sem_id_epochs': [9986, 9987, 9988, 9989, 9990, 9991, 9992, 9993, 9994, 9995, 9996, 9997, 9998, 9999, 10000], 'run_local_time': 'Nov-20-2025_01-19', 'ckpt_name': 'Nov-20-2025_01-19-452d96', 'dataset': 'Industrial_and_Scientific', 'device': device(type='cuda'), 'use_ddp': False, 'accelerator': <accelerate.accelerator.Accelerator object at 0x7dde0e3b2bf0>}
Thu 20 Nov 2025 01:19:51 INFO  [TOKENIZER] Loading semantic IDs from datasets/Industrial_and_Scientific/rqvae/sentence-t5-base_256,256,256,256_9986.sem_ids...
Thu 20 Nov 2025 01:19:51 INFO  [TOKENIZER] Loading semantic IDs from datasets/Industrial_and_Scientific/rqvae/sentence-t5-base_256,256,256,256_9987.sem_ids...
Thu 20 Nov 2025 01:19:51 INFO  [TOKENIZER] Loading semantic IDs from datasets/Industrial_and_Scientific/rqvae/sentence-t5-base_256,256,256,256_9988.sem_ids...
Thu 20 Nov 2025 01:19:51 INFO  [TOKENIZER] Loading semantic IDs from datasets/Industrial_and_Scientific/rqvae/sentence-t5-base_256,256,256,256_9989.sem_ids...
Thu 20 Nov 2025 01:19:51 INFO  [TOKENIZER] Loading semantic IDs from datasets/Industrial_and_Scientific/rqvae/sentence-t5-base_256,256,256,256_9990.sem_ids...
Thu 20 Nov 2025 01:19:52 INFO  [TOKENIZER] Loading semantic IDs from datasets/Industrial_and_Scientific/rqvae/sentence-t5-base_256,256,256,256_9991.sem_ids...
Thu 20 Nov 2025 01:19:52 INFO  [TOKENIZER] Loading semantic IDs from datasets/Industrial_and_Scientific/rqvae/sentence-t5-base_256,256,256,256_9992.sem_ids...
Thu 20 Nov 2025 01:19:52 INFO  [TOKENIZER] Loading semantic IDs from datasets/Industrial_and_Scientific/rqvae/sentence-t5-base_256,256,256,256_9993.sem_ids...
Thu 20 Nov 2025 01:19:52 INFO  [TOKENIZER] Loading semantic IDs from datasets/Industrial_and_Scientific/rqvae/sentence-t5-base_256,256,256,256_9994.sem_ids...
Thu 20 Nov 2025 01:19:52 INFO  [TOKENIZER] Loading semantic IDs from datasets/Industrial_and_Scientific/rqvae/sentence-t5-base_256,256,256,256_9995.sem_ids...
Thu 20 Nov 2025 01:19:52 INFO  [TOKENIZER] Loading semantic IDs from datasets/Industrial_and_Scientific/rqvae/sentence-t5-base_256,256,256,256_9996.sem_ids...
Thu 20 Nov 2025 01:19:52 INFO  [TOKENIZER] Loading semantic IDs from datasets/Industrial_and_Scientific/rqvae/sentence-t5-base_256,256,256,256_9997.sem_ids...
Thu 20 Nov 2025 01:19:52 INFO  [TOKENIZER] Loading semantic IDs from datasets/Industrial_and_Scientific/rqvae/sentence-t5-base_256,256,256,256_9998.sem_ids...
Thu 20 Nov 2025 01:19:53 INFO  [TOKENIZER] Loading semantic IDs from datasets/Industrial_and_Scientific/rqvae/sentence-t5-base_256,256,256,256_9999.sem_ids...
Thu 20 Nov 2025 01:19:53 INFO  [TOKENIZER] Loading semantic IDs from datasets/Industrial_and_Scientific/rqvae/sentence-t5-base_256,256,256,256_10000.sem_ids...
Thu 20 Nov 2025 01:19:55 INFO  MTGRec(
  (t5): T5ForConditionalGeneration(
    (shared): Embedding(1027, 128)
    (encoder): T5Stack(
      (embed_tokens): Embedding(1027, 128)
      (block): ModuleList(
        (0): T5Block(
          (layer): ModuleList(
            (0): T5LayerSelfAttention(
              (SelfAttention): T5Attention(
                (q): Linear(in_features=128, out_features=256, bias=False)
                (k): Linear(in_features=128, out_features=256, bias=False)
                (v): Linear(in_features=128, out_features=256, bias=False)
                (o): Linear(in_features=256, out_features=128, bias=False)
                (relative_attention_bias): Embedding(32, 4)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): T5LayerFF(
              (DenseReluDense): T5DenseActDense(
                (wi): Linear(in_features=128, out_features=512, bias=False)
                (wo): Linear(in_features=512, out_features=128, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (act): ReLU()
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1-4): 4 x T5Block(
          (layer): ModuleList(
            (0): T5LayerSelfAttention(
              (SelfAttention): T5Attention(
                (q): Linear(in_features=128, out_features=256, bias=False)
                (k): Linear(in_features=128, out_features=256, bias=False)
                (v): Linear(in_features=128, out_features=256, bias=False)
                (o): Linear(in_features=256, out_features=128, bias=False)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): T5LayerFF(
              (DenseReluDense): T5DenseActDense(
                (wi): Linear(in_features=128, out_features=512, bias=False)
                (wo): Linear(in_features=512, out_features=128, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (act): ReLU()
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (final_layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (decoder): T5Stack(
      (embed_tokens): Embedding(1027, 128)
      (block): ModuleList(
        (0): T5Block(
          (layer): ModuleList(
            (0): T5LayerSelfAttention(
              (SelfAttention): T5Attention(
                (q): Linear(in_features=128, out_features=256, bias=False)
                (k): Linear(in_features=128, out_features=256, bias=False)
                (v): Linear(in_features=128, out_features=256, bias=False)
                (o): Linear(in_features=256, out_features=128, bias=False)
                (relative_attention_bias): Embedding(32, 4)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): T5LayerCrossAttention(
              (EncDecAttention): T5Attention(
                (q): Linear(in_features=128, out_features=256, bias=False)
                (k): Linear(in_features=128, out_features=256, bias=False)
                (v): Linear(in_features=128, out_features=256, bias=False)
                (o): Linear(in_features=256, out_features=128, bias=False)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (2): T5LayerFF(
              (DenseReluDense): T5DenseActDense(
                (wi): Linear(in_features=128, out_features=512, bias=False)
                (wo): Linear(in_features=512, out_features=128, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (act): ReLU()
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1-4): 4 x T5Block(
          (layer): ModuleList(
            (0): T5LayerSelfAttention(
              (SelfAttention): T5Attention(
                (q): Linear(in_features=128, out_features=256, bias=False)
                (k): Linear(in_features=128, out_features=256, bias=False)
                (v): Linear(in_features=128, out_features=256, bias=False)
                (o): Linear(in_features=256, out_features=128, bias=False)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): T5LayerCrossAttention(
              (EncDecAttention): T5Attention(
                (q): Linear(in_features=128, out_features=256, bias=False)
                (k): Linear(in_features=128, out_features=256, bias=False)
                (v): Linear(in_features=128, out_features=256, bias=False)
                (o): Linear(in_features=256, out_features=128, bias=False)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (2): T5LayerFF(
              (DenseReluDense): T5DenseActDense(
                (wi): Linear(in_features=128, out_features=512, bias=False)
                (wo): Linear(in_features=512, out_features=128, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (act): ReLU()
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (final_layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (lm_head): Linear(in_features=128, out_features=1027, bias=False)
  )
)
Thu 20 Nov 2025 01:19:55 INFO  #Embedding parameters: 131456
#Non-embedding parameters: 3280512
#Total trainable parameters: 3411968

Thu 20 Nov 2025 01:21:04 INFO  [Epoch 1] Train Loss: 3.934067329083841
Thu 20 Nov 2025 01:22:06 INFO  [Epoch 2] Train Loss: 2.8833061721850566
Thu 20 Nov 2025 01:23:08 INFO  [Epoch 3] Train Loss: 2.644895129785763
Thu 20 Nov 2025 01:24:10 INFO  [Epoch 4] Train Loss: 2.5254794460112655
Thu 20 Nov 2025 01:25:13 INFO  [Epoch 5] Train Loss: 2.4432468064657344
Thu 20 Nov 2025 01:26:14 INFO  [Epoch 6] Train Loss: 2.387918414562706
