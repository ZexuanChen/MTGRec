Mon 24 Nov 2025 23:42:20 INFO  Device: cuda
Mon 24 Nov 2025 23:42:20 INFO  Config: {'rand_seed': 2024, 'reproducibility': True, 'num_proc': 1, 'data_dir': 'datasets/Industrial_and_Scientific', 'log_dir': 'run_logs/', 'tensorboard_log_dir': 'tensorboard/', 'ckpt_dir': 'ckpt/Industrial_and_Scientific/Nov-24-2025_23-42-d3b57a', 'stage': 'finetune', 'pretrained_model': './ckpt/Industrial_and_Scientific/Nov-19-2025_22-00-ba0854_120.pth', 'train_batch_size': 256, 'eval_batch_size': 128, 'lr': 0.0002, 'weight_decay': 0.05, 'warmup_steps': 0, 'steps': None, 'epochs': 100, 'max_grad_norm': 1.0, 'eval_interval': 1, 'save_interval': None, 'patience': 10, 'topk': [5, 10], 'metrics': ['ndcg', 'recall'], 'val_metric': 'ndcg@10', 'val_ratio': 1.0, 'val_delay': 0, 'n_codebooks': 3, 'codebook_size': 256, 'expand_final': True, 'token_prefix': 'rqvae/sentence-t5-base_256,256,256,256', 'token_suffix': 'sem_ids', 'n_user_tokens': 1, 'max_item_seq_len': 20, 'num_beams': 20, 'test_num_beams': 50, 'num_layers': 5, 'num_decoder_layers': 5, 'd_model': 128, 'd_ff': 512, 'num_heads': 4, 'd_kv': 64, 'dropout_rate': 0.1, 'activation_function': 'relu', 'feed_forward_proj': 'relu', 'results_dir': None, 'sem_id_epochs': [9991, 9992, 9993, 9994, 9995], 'run_local_time': 'Nov-24-2025_23-42', 'ckpt_name': 'Nov-24-2025_23-42-d3b57a', 'dataset': 'Industrial_and_Scientific', 'device': device(type='cuda'), 'use_ddp': True, 'accelerator': <accelerate.accelerator.Accelerator object at 0x743c453a3af0>}
Mon 24 Nov 2025 23:42:20 INFO  [TOKENIZER] Loading semantic IDs from datasets/Industrial_and_Scientific/rqvae/sentence-t5-base_256,256,256,256_9991.sem_ids...
Mon 24 Nov 2025 23:42:20 INFO  [TOKENIZER] Loading semantic IDs from datasets/Industrial_and_Scientific/rqvae/sentence-t5-base_256,256,256,256_9992.sem_ids...
Mon 24 Nov 2025 23:42:20 INFO  [TOKENIZER] Loading semantic IDs from datasets/Industrial_and_Scientific/rqvae/sentence-t5-base_256,256,256,256_9993.sem_ids...
Mon 24 Nov 2025 23:42:20 INFO  [TOKENIZER] Loading semantic IDs from datasets/Industrial_and_Scientific/rqvae/sentence-t5-base_256,256,256,256_9994.sem_ids...
Mon 24 Nov 2025 23:42:20 INFO  [TOKENIZER] Loading semantic IDs from datasets/Industrial_and_Scientific/rqvae/sentence-t5-base_256,256,256,256_9995.sem_ids...
Mon 24 Nov 2025 23:42:22 INFO  MTGRec(
  (t5): T5ForConditionalGeneration(
    (shared): Embedding(1027, 128)
    (encoder): T5Stack(
      (embed_tokens): Embedding(1027, 128)
      (block): ModuleList(
        (0): T5Block(
          (layer): ModuleList(
            (0): T5LayerSelfAttention(
              (SelfAttention): T5Attention(
                (q): Linear(in_features=128, out_features=256, bias=False)
                (k): Linear(in_features=128, out_features=256, bias=False)
                (v): Linear(in_features=128, out_features=256, bias=False)
                (o): Linear(in_features=256, out_features=128, bias=False)
                (relative_attention_bias): Embedding(32, 4)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): T5LayerFF(
              (DenseReluDense): T5DenseActDense(
                (wi): Linear(in_features=128, out_features=512, bias=False)
                (wo): Linear(in_features=512, out_features=128, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (act): ReLU()
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1-4): 4 x T5Block(
          (layer): ModuleList(
            (0): T5LayerSelfAttention(
              (SelfAttention): T5Attention(
                (q): Linear(in_features=128, out_features=256, bias=False)
                (k): Linear(in_features=128, out_features=256, bias=False)
                (v): Linear(in_features=128, out_features=256, bias=False)
                (o): Linear(in_features=256, out_features=128, bias=False)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): T5LayerFF(
              (DenseReluDense): T5DenseActDense(
                (wi): Linear(in_features=128, out_features=512, bias=False)
                (wo): Linear(in_features=512, out_features=128, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (act): ReLU()
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (final_layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (decoder): T5Stack(
      (embed_tokens): Embedding(1027, 128)
      (block): ModuleList(
        (0): T5Block(
          (layer): ModuleList(
            (0): T5LayerSelfAttention(
              (SelfAttention): T5Attention(
                (q): Linear(in_features=128, out_features=256, bias=False)
                (k): Linear(in_features=128, out_features=256, bias=False)
                (v): Linear(in_features=128, out_features=256, bias=False)
                (o): Linear(in_features=256, out_features=128, bias=False)
                (relative_attention_bias): Embedding(32, 4)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): T5LayerCrossAttention(
              (EncDecAttention): T5Attention(
                (q): Linear(in_features=128, out_features=256, bias=False)
                (k): Linear(in_features=128, out_features=256, bias=False)
                (v): Linear(in_features=128, out_features=256, bias=False)
                (o): Linear(in_features=256, out_features=128, bias=False)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (2): T5LayerFF(
              (DenseReluDense): T5DenseActDense(
                (wi): Linear(in_features=128, out_features=512, bias=False)
                (wo): Linear(in_features=512, out_features=128, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (act): ReLU()
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1-4): 4 x T5Block(
          (layer): ModuleList(
            (0): T5LayerSelfAttention(
              (SelfAttention): T5Attention(
                (q): Linear(in_features=128, out_features=256, bias=False)
                (k): Linear(in_features=128, out_features=256, bias=False)
                (v): Linear(in_features=128, out_features=256, bias=False)
                (o): Linear(in_features=256, out_features=128, bias=False)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): T5LayerCrossAttention(
              (EncDecAttention): T5Attention(
                (q): Linear(in_features=128, out_features=256, bias=False)
                (k): Linear(in_features=128, out_features=256, bias=False)
                (v): Linear(in_features=128, out_features=256, bias=False)
                (o): Linear(in_features=256, out_features=128, bias=False)
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (2): T5LayerFF(
              (DenseReluDense): T5DenseActDense(
                (wi): Linear(in_features=128, out_features=512, bias=False)
                (wo): Linear(in_features=512, out_features=128, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (act): ReLU()
              )
              (layer_norm): T5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (final_layer_norm): T5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (lm_head): Linear(in_features=128, out_features=1027, bias=False)
  )
)
Mon 24 Nov 2025 23:42:22 INFO  #Embedding parameters: 131456
#Non-embedding parameters: 3280512
#Total trainable parameters: 3411968

Mon 24 Nov 2025 23:42:47 INFO  [Epoch 1] Train Loss: 4.036029862091604
Mon 24 Nov 2025 23:45:16 INFO  [Epoch 1] Val Results: OrderedDict([('ndcg@5', np.float64(0.0053421749733388426)), ('ndcg@10', np.float64(0.006132094655185938)), ('recall@5', np.float64(0.005778170190751553)), ('recall@10', np.float64(0.00821025762706995))])
Mon 24 Nov 2025 23:45:16 INFO  [Epoch 1] Val_0 Results: OrderedDict([('ndcg@5', 0.005348904989659786), ('ndcg@10', 0.006162604782730341), ('recall@5', 0.005766401998698711), ('recall@10', 0.008276944048702717)])
Mon 24 Nov 2025 23:45:16 INFO  [Epoch 1] Val_1 Results: OrderedDict([('ndcg@5', 0.005350113846361637), ('ndcg@10', 0.006129041779786348), ('recall@5', 0.005786015652120113), ('recall@10', 0.008178875781595707)])
Mon 24 Nov 2025 23:45:16 INFO  [Epoch 1] Val_2 Results: OrderedDict([('ndcg@5', 0.005314813926815987), ('ndcg@10', 0.006107834167778492), ('recall@5', 0.005746788345277309), ('recall@10', 0.008178875781595707)])
Mon 24 Nov 2025 23:45:16 INFO  [Epoch 1] Val_3 Results: OrderedDict([('ndcg@5', 0.005358095280826092), ('ndcg@10', 0.0061157033778727055), ('recall@5', 0.005805629305541515), ('recall@10', 0.008139648474752903)])
Mon 24 Nov 2025 23:45:16 INFO  [Epoch 1] Val_4 Results: OrderedDict([('ndcg@5', 0.00533894682303071), ('ndcg@10', 0.006145289167761803), ('recall@5', 0.005786015652120113), ('recall@10', 0.008276944048702717)])
