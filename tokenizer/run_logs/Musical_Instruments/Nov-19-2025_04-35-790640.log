Wed 19 Nov 2025 04:35:17 INFO  Device: cuda
Wed 19 Nov 2025 04:35:17 INFO  Config: {'data_dir': '../datasets/Musical_Instruments', 'log_dir': 'run_logs/', 'rand_seed': 2024, 'reproducibility': True, 'lr': 0.001, 'learner': 'adagrad', 'scheduler_type': 'constant', 'weight_decay': 0.0, 'warmup_steps': 0, 'batch_size': 2048, 'epochs': 10000, 'verbose_step': 100, 'verbose_delay': 8000, 'save_limit': 100, 'ckpt_name': 'rqvae', 'sent_emb_model': 'sentence-transformers/sentence-t5-base', 'sent_emb_batch_size': 512, 'sent_emb_dim': 768, 'sent_emb_pca': 128, 'n_codebooks': 3, 'codebook_size': 256, 'hidden_sizes': [2048, 1024, 512, 256, 128], 'dropout': 0.0, 'beta': 0.25, 'vq_type': 'ema', 'run_local_time': 'Nov-19-2025_04-35', 'dataset': 'Musical_Instruments', 'device': device(type='cuda'), 'use_ddp': False, 'accelerator': <accelerate.accelerator.Accelerator object at 0x799db4306ec0>}
Wed 19 Nov 2025 04:35:17 INFO  [TOKENIZER] Encoding sentence embeddings...
Wed 19 Nov 2025 04:35:17 INFO  Use pytorch device_name: cuda:0
Wed 19 Nov 2025 04:35:17 INFO  Load pretrained SentenceTransformer: sentence-transformers/sentence-t5-base
Wed 19 Nov 2025 04:36:24 INFO  [TOKENIZER] Sentence embeddings shape: (24587, 128)
Wed 19 Nov 2025 04:36:24 INFO  [TOKENIZER] Sentence embeddings shape after filtering: (24556, 128)
Wed 19 Nov 2025 04:36:24 INFO  RQVAEModel(
  (encoder): MLP(
    (mlp): Sequential(
      (0): Dropout(p=0.0, inplace=False)
      (1): Linear(in_features=128, out_features=2048, bias=True)
      (2): ReLU()
      (3): Dropout(p=0.0, inplace=False)
      (4): Linear(in_features=2048, out_features=1024, bias=True)
      (5): ReLU()
      (6): Dropout(p=0.0, inplace=False)
      (7): Linear(in_features=1024, out_features=512, bias=True)
      (8): ReLU()
      (9): Dropout(p=0.0, inplace=False)
      (10): Linear(in_features=512, out_features=256, bias=True)
      (11): ReLU()
      (12): Dropout(p=0.0, inplace=False)
      (13): Linear(in_features=256, out_features=128, bias=True)
    )
  )
  (quantization_layer): RQLayer(
    (quantization_layers): ModuleList(
      (0-2): 3 x EMAVQLayer()
    )
  )
  (decoder): MLP(
    (mlp): Sequential(
      (0): Dropout(p=0.0, inplace=False)
      (1): Linear(in_features=128, out_features=256, bias=True)
      (2): ReLU()
      (3): Dropout(p=0.0, inplace=False)
      (4): Linear(in_features=256, out_features=512, bias=True)
      (5): ReLU()
      (6): Dropout(p=0.0, inplace=False)
      (7): Linear(in_features=512, out_features=1024, bias=True)
      (8): ReLU()
      (9): Dropout(p=0.0, inplace=False)
      (10): Linear(in_features=1024, out_features=2048, bias=True)
      (11): ReLU()
      (12): Dropout(p=0.0, inplace=False)
      (13): Linear(in_features=2048, out_features=128, bias=True)
    )
  )
)
